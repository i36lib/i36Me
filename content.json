{"pages":[{"title":"关于我","text":"阿北，2013年毕业于云南大学国家示范性软件学院. &gt; 后端研发工程师，擅长Java/Python. &gt; 常跑步，爱好编程，喜欢听民谣，睡懒觉. &gt; 目前就职于AAM中国（ArtsAllianceMedia），任职高级研发工程师. 联系我 在blog留言 Wechat: i03020 Email: i36.lib#gmail.com","link":"/about/index.html"}],"posts":[{"title":"C/C++程序的内存布局与内存泄漏","text":"C语言很接近于硬件，是说相比于具有自动内存管理等机制的Java等高层面的程序设计语言，C程序员对于一点一滴内存都需要自己管理，这就要求思维需要很紧密，要清晰地了解内存结构和程序的工作机制。在写C/C++程序的时候，稍不注意，就有可能导致内存泄漏了，所以，知道C程序的内存布局和工作机制，是非常有必要的。后面举了一个例子，它展示了在某种情况下将指针初始化为同一个值将导致的内存泄漏问题，从该例子来窥探指针是怎么工作的，为什么导致了内存泄漏。 C程序内存结构一个典型的C程序在内存中的布局大致会包括以下几个部分，各个部分按内存地址从地到高依次为： 代码段 数据段 BSS段 堆栈段 如下图左边部分所示： 代码段又称为文本段。它包含程序的执行指令内容，它一般会被安排在低地址部分，这样做是为了避免产生堆栈段在分配动态内存时可能产生的内存溢出会覆盖这部分内存的危险。这部分内容通常是只读的，但也有可能是共享的。 数据段指已初始化数据段，这部分包含了已初始化的全局变量和静态变量。这部分内存并不完全是只读的，如字符串常量值会被存放在只读部分，而可变更的变量则会被存放在可读/写区。 BSS段BSS指的是Block Started by Symbol，由符号开始的段。BSS段指未初始化数据段，包括了没有明确初始化的全局变量和静态变量。这些变量在程序开始运行的时候将由程序内核负责初始化为0或者为空。如一个未初始化的静态变量static int a;未初始化的全局变量int b;将存放在该段。 堆栈段包括了栈(stack)和堆(heap)两部分。不知道哪个SB在写书的时候将stack翻译为堆栈，这是一个非常误人子弟的翻译。栈和堆都是程序在运行的时候可以动态分配使用的内存部分，但是栈由操作系统内核来使用，而允许程序员使用的部分是堆，即我们所调用的malloc()系内存分配所得到的内存是在堆上的。 内存泄漏： 内存泄漏指分配的内存在使用过后不做释放，导致不能重复利用，除非程序终止，否则这部分内存就再也不能被使用。若程序一直申请内存而不释放，久而久之将使可用内存变少，进而支持虚拟内存的系统出现非常频繁的段页交换，使得系统运行变慢。看一个例子(伪码)是我在写base64编码/解码的测试用例的时候遇到的。 123456789101112131415161718192021222324252627282930#include &lt;stdio.h&gt;/* int b64_encode(char *source, char **encoded); *//* int b64_decode(char *source, char **decoded); *//* @return: return the size of encoded/decoded string *//* Use void b64_free(char **ptr); to free ptr */int main(){ int rval = 0; char *init = \"Temp\"; char **ptr1, **ptr2; char *str = \"This string is to be test.\"; ptr1 = ptr2 = &amp;init; /* init to a save value */ printf(\"The string is: %s\\n\", str); rval = b64_encode(str, ptr1); printf(\"encoded: %s\\n\", *ptr1); /* memory leak here: */ rval = b64_decode(*ptr1, ptr2); printf(\"decoded: %s\\n\", *ptr2) b64_free(ptr1); b64_free(ptr2); /* error */ return 0;} 在这个例子中，有两个函数，分别做base64编码的编码和解码工作，函数返回编码/解码后的字符串长度，然后在参数中使用二维指针来返回得到的编码/解码结果，结果是在函数内部根据结果字符串长度来动态分配内存存储的。这里在编码之后直接使用编码结果作为解码函数的输入来执行解码，然后最后26、27行来释放这两个指针。乍一看，好像代码挺对的，没啥问题，但实际上第27行出错了，提示内存已经释放了！靠，为毛？我们再来看一眼程序在内存中的布局，我将上述代码的一部分画入了下图右边部分： 从上图可以看到，在代码16行，将两个二维指针初始化成为了同一个值，意味着什么呢？看图，意味着这两个指针指向了同一个“中转站”即init这个指针，函数在被调用之前，ptr1和ptr2并不知道即将申请的内存的地址(因为还没申请嘛！)它俩只知道“中转站”init在哪儿，函数在调用之后申请了内存，将地址填入init中，然后ptr1和ptr2再从“中转站”中得到新申请的内存的地址，从而读取编码/解码的结果。注意：由于这里使用了同一个“中转站”(一维指针)，在第23行代码执行之前，“中转站”init指针存放的是19行编码函数内申请的内存地址，而在23行解码函数调用之后，“中转站”init中的地址就被覆盖了！！！也就是说第一个编码函数内申请的内存到了这里没有人再知道它的地址是什么了，等到了26行去freeptr1的时候，释放的是23行解码函数内申请的那块内存，而27行再释放ptr2时，ptr2指向的“中转站”init的值也就是第二次申请的那块内存刚刚在26行被释放了，于是这里报错，无法释放内存。 好了，如果b64_free里面将行参值置空会怎么样？即在b64_free函数内在释放内存后加一句*ptr = NULL;那么结果就是第27行代码不会报错了，但是内存还是泄漏了。嗯，如果你使用splint程序检验你的代码，就能发现这个问题。但如果了解这背后的机理，那么很多问题也就不会产生了。 函数及其调用这里讲讲我对函数及其调用的理解： 函数是一段可执行代码，这段代码内的变量在被调用时由系统在栈(stack)上动态分配存储空间(静态变量除外)，这段执行代码指令编译后存放于代码段当中，它有一个起始地址，即为函数入口地址，可以声明一个指针来指向这个入口地址，称这个指针为函数指针，说到函数指针就要说到指针函数，指针函数指的是返回值是指针的函数。函数指针是一个指针，而指针是变量，它可以指向其他函数的入口地址，但要求函数的返回类型、行参类型要相同。声明了函数指针，这样就可以在运行时动态地根据需要指向不同的函数，到了C++里面，这个机制就称之为多态。 例如有一个类Father，它有一个虚函数(用virtual关键字声明)叫speek();它有一个子类叫Son，Son也有一个函数叫speek()。当你用Father类声明一个对象obj，但给它分配的内存类型却是Son的，那么在你调用obj.speek()的时候这个隐含的函数指针就会给你指向Son的speek()实现，但是如果Son没有实现这个speek()函数，这个调用就会指向Father的实现，而如果没有使用virtual关键字，那么这个”多态性”就消失了，即virtual关键字就是为了告诉编译器选择合适的一个实现来调用。这其中就暗含着函数指针的运作，只是C++给我们抽象了，编译器来帮忙完成了这个事情，我们需要做的只是加个virtual关键字以及做同名的不同实现。 那么，函数调用是怎么样的呢，由图上看，当一个函数被调用的时候，系统根据入口地址找到函数代码段来开始执行，同时为其返回值变量分配一个存储空间，为其中用到的局部变量和形参变量也分配存储空间，这些存储空间是在栈中分配的。各次函数调用都会分配空间，但静态变量是在数据段/BSS段中分配，所以会被各次函数调用所“共享”。之后将实参的值拷贝给栈中的形参变量（由此你可以知道C语言中只有一种传参方式，就是传值，指针本身也是变量，值是地址而已；但到了C++就多了一个传引用，这意味着实际上函数调用时不会为形参在栈中分配空间，而是在函数执行需要用到形参时直接去读取实参的值，这也意味着在C语言中外部变量在函数内部不能使用的规则给“打破”了，实参实际上通过引用这种方式作用到了函数内部。）然后执行完毕后将返回值(即图中X,Y部分)的值拷贝给函数调用的接收者，进而这些在栈中分配的局部变量存储空间被释放回收，函数生命周期结束。 特别地，多线程时并发的情况存在，当函数是一个线程函数并且被多个线程共用时，这时候如果存在静态变量，虽然函数每次调用都会分配新的空间，但由于静态变量共享，这个时候多个线程共享一个变量，这将使得程序逻辑不再局部于线程本身，而极大可能会被其他线程影响和修改结果，这个就是线程安全问题，这个函数就是线程不安全的。 参考资料 《C Traps and Pitfalls》 《Expert C Programming》 《The C Programming Language》 《Programming Principles and Practice Using C++》","link":"/2015/10/06/98e684fc1797454ca.html"},{"title":"C/C++结构体的内存对齐","text":"内存对齐是编译器做的事情，但程序员如果明白其中的原理，将有助于写出更好的程序。在C/C++的结构体(Struct)当中，各个不同类型的成员的先后排列次序极大的可能会影响到该结构体占用内存空间的大小。先来看一个例子，在这个例子当中定义两个结构体，里面的成员是一样的，区别只在于排列顺序不同。 一个例子1234567891011121314151617181920212223242526272829303132#include &lt;stdio.h&gt;/* #pragma pack(8) *//* Under x86-64bit CPU, Fedora22, GCC, default pack(8) */int main(int argc, char *argv[]){ struct Test1{ /* logical address start with 0 */ int ivar1; /* map address 0, size 4 Bytes */ char cvar1; /* map address 4, size 1 Bytes */ int ivar2; /* map address 8, size 4 Bytes */ double dvar1; /* map address 16, size 8 Bytes */ int *pivar; /* map address 24, size 8 Bytes(8x8bit=64bit) */ float *pfvar; /* map address 32, size 8 Bytes(8x8bit=64bit) */ char cvar2; /* map address 40, size 1 Bytes */ }; /* End of address 41, size: (41/8)*8+41%8=48 Bytes */ struct Test2 { /* logical address start with 0 */ char cvar1; /* map address 0, size 1 Byte */ char cvar2; /* map address 1, size 1 Byte */ int ivar1; /* map address 4, size 4 Bytes */ int ivar2; /* map address 8, size 4 Bytes */ double dvar1; /* map address 16, size 8 Bytes */ int *pivar; /* map address 24, size 8 Bytes */ float *pfvar; /* map address 32, size 8 Bytes */ }; /* End of address 40, size: (40/8)*8+40%8=40 Bytes */ printf(\"Size of struct Test1: %d Bytes.\\n\", sizeof(struct Test1)); printf(\"Size of struct Test2: %d Bytes.\\n\", sizeof(struct Test2)); return 0;} /* main */ 在Fedora22/Gcc下的测试： 12345[fury@localhost clabs]$ gcc -o exe struct.c -g[fury@localhost clabs]$ ./exeSize of struct Test1: 48 Bytes.Size of struct Test2: 40 Bytes.[fury@localhost clabs]$ 可以看到，排列顺序的不同使得内存占用大小相差了8个字节。或者将结构体这么排列，虽然结果还是40Bytes，但成员的对齐已经发生变化了，经过对比或许你能理解得更清晰： 123456789struct Test3 { /* logical address start with 0 */ int ivar1; /* map address 0, size 4 Bytes */ int ivar2; /* map address 4, size 4 Bytes */ double dvar1; /* map address 8, size 8 Bytes */ int *pivar; /* map address 16, size 8 Bytes */ float *pfvar; /* map address 24, size 8 Bytes */ char cvar1; /* map address 32, size 1 Byte */ char cvar2; /* map address 33, size 1 Byte */}; /* End of address 34, size: (34/8)*8+34%8=40 Bytes */ 到了这里，依靠代码的注释或许你已经看出一些端倪了。先解释几个关键点，然后再结合上面的代码例子来分析。 内存编址以x86架构CPU为例。内存基本单元的大小是1Byte(8bit)，若是32位机即数据总线宽度为32位，可同时传输32个0/1位，以一个字节编一个地址，根据排列组合原理就有2^32个地址，每个地址是一个内存单元，大小为1Byte(8bit)，则该CPU最大内存寻址能力将为2^32 x 8bit = 4 x 2^30 x 1Byte = 4 x 1G x 1Byte = 4GBytes(即4GB)。这就是通常所说的32机理论可使用最大内存为4GB，同理64位机就是2^64字节。 对齐系数每个编译器都有自己默认的对齐系数。以x86-64/Fedora22为例，gcc的默认对齐系数是8，为什么是8呢？因为机器是64位，而8位是一个字节，64bit/8bit=8Bytes，也就是说它一下子可以取8个字节。所以若是32位机，那么最佳的对齐系数应该为4。可以在代码中使用#pragma pack(n) /* n取1,2,4,8,16即2的整次幂 */来更改编译器默认的对齐系数。 类型大小 char：1 Byte. short：2 Bytes. int：4 Bytes. long：4 Bytes. —— 32位机. long：8 Bytes. —— 64位机. float：4 Bytes. double：8 Bytes. long long：8 Bytes. long double：8 Bytes(VC++)，12 Bytes(GCC). —— 32位机. long double：8 Bytes(VC++)，16 Bytes(GCC). —— 64位机. pointer：4 Bytes. —— 32位机. pointer：8 Bytes. —— 64位机. 成员对齐指结构体中数据成员的地址对齐。假定结构体的起始逻辑地址为0，各成员的偏移地址按顺序对齐，第一个成员对齐到0，接下来的成员对齐到Min(成员类型大小，对齐系数)的倍数地址。如在对齐系数为8的情况下，char可以对齐到任意一个地址，int应该对齐到4n(n取自然数)，32位机指针对齐到4n，64位机指针对齐到8n，float对齐到4n，double对齐到8n，等等。 结构对齐指结构体整体对齐。结构体的结束地址要对齐到Min(最大成员类型大小，对齐系数)的倍数地址。如对齐系数为8，假定结构体内只有char、int两种类型，则最大的类型为int即占据内存大小最大的类型为4字节，而Min(4,8)=4，所以结构体结束地址应该为4n(n取整数，n&gt;1)。 下面以Test1为例来分析代码(对齐系数为8)，Test2/Test3是一样的道理。 代码分析 int ivar1： 第一个成员，它将对齐到min(4,8)*n，对齐到地址0； char cvar1： 由于上一个成员占据了4个字节，所以它对齐到min(1,8)*n，对齐到地址4； int ivar2： 上一个成员占据1个字节，而5将是其最小开始地址，但要对齐到min(4,8)*n，所以这里将会对齐到地址8，于是你便可以知道在char cvar1之后，空出了3个字节空间； double dvar1： 由上一个成员的分析，同理可知它将对齐到min(8,8)*n，对齐到地址16，前面空出4个字节； int *pivar： 64位机器内指针为8字节，上一个成员占据8字节，它对齐到min(8,8)*n，对齐到地址24； float *pfvar： 也是指针，对齐到min(8,8)*n，对齐到地址32； char cvar2： 大小为1字节，上一个大小为8字节，这里对齐到min(1,8)*n，对齐到地址40； struct Test1： 最后一个成员大小为1字节，结构体结束于41。即到目前为止结构体已经占据从0到40共41个字节，开始做整体对齐，它应该对齐到min(max(member type size), 8)*n即min(8,8)*n，对齐到48，因为41往后8的最小倍数就是48；于是结构体占据的空间为0到47共48个字节，最后空出7个字节。 参照代码注释，即可同理分析struct Test2和struct Test3。注意：有一种理解误区是，单纯地将各个成员大小相加然后做整体对齐，这是错误的。关键点在于要理解是每个成员以及结构体本身的地址要映射到内存地址。通过更改对齐系数，会改变结构体占据内存的大小，很显然，当对齐系数为1的时候，大小就直接是各个成员大小之和了，但这样不见得会使程序的效率提高。 参考资料 Data Structure Alignment Memory Layout of C Programs Data alignment: Straighten up and fly right","link":"/2015/10/06/4596d72ad01f47bf.html"},{"title":"Java8 ArrayList<E>源码解读","text":"ArrayList&lt;E&gt;是能够自动扩容版本的数组实现，它继承了​AbstractList&lt;E&gt;​抽象类，实现了​List&lt;E&gt;, RandomAccess, Cloneable, Serializable​等几个接口： 12public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable ArrayList&lt;E&gt;初探 ArrayList&lt;E&gt;​内部以一个​Object​数组存储元素，以一个整型​size​成员记录元素的个数： 123456private static final int DEFAULT_CAPACITY = 10;private static final Object[] EMPTY_ELEMENTDATA = {};private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {};transient Object[] elementData; //transient表名了elementData不参与序列化private int size;protected transient int modCount = 0; // 从AbstractList&lt;E&gt;继承 其有三个构造函数： 无参构造函数： 123public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;} 初始化的是一个空对象数组，其实际数组长度此时为0 指定初始容量的构造函数 12345678910public ArrayList(int initialCapacity) { if (initialCapacity &gt; 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_ELEMENTDATA; } else { throw new IllegalArgumentException(\"Illegal Capacity: \"+ initialCapacity); }} initialCapacity​大于0时，初始化的是一个参数指定容量长度的对象数组 从另一个集合对象构造ArrayList&lt;E&gt;​ 123456789101112public ArrayList(Collection&lt;? extends E&gt; c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // defend against c.toArray (incorrectly) not returning Object[] // (see e.g. https://bugs.openjdk.java.net/browse/JDK-6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; }} 从参数集合对象拷贝成对象数组，其长度是原集合对象的长度 modCount​从​AbstractList&lt;E&gt;继承而来，用来标记​ArrayList&lt;E&gt;​实例被结构性改变（如元素删除等）的次数，其被用来在迭代器中判断迭代过程中数组是否被做了修改，如果被修改则抛出​ConcurrentModificationException​。 延迟初始化与扩容 可以看到对于ArrayList&lt;E&gt;​的默认构造函数，并没有在对象构造的时候就初始化一定的空间容量，而是延迟到了往里面添加元素或者调用​ensureCapacity(int)​的时候才去分配容量： 123456789101112131415161718192021222324private void add(E e, Object[] elementData, int s) { if (s == elementData.length) elementData = grow(); elementData[s] = e; size = s + 1;}public boolean add(E e) { modCount++; add(e, elementData, size); return true;}public void add(int index, E element) { rangeCheckForAdd(index); modCount++; final int s; Object[] elementData; if ((s = size) == (elementData = this.elementData).length) elementData = grow(); System.arraycopy(elementData, index, elementData, index + 1, s - index); elementData[index] = element; size = s + 1;} 其中的grow()​方法即是做自动扩容的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748private Object[] grow(int minCapacity) { int oldCapacity = elementData.length; if (oldCapacity &gt; 0 || elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { int newCapacity = ArraysSupport.newLength(oldCapacity, minCapacity - oldCapacity, /* minimum growth */ oldCapacity &gt;&gt; 1 /* preferred growth */); return elementData = Arrays.copyOf(elementData, newCapacity); } else { return elementData = new Object[Math.max(DEFAULT_CAPACITY, minCapacity)]; }}private Object[] grow() { return grow(size + 1);}public void ensureCapacity(int minCapacity) { if (minCapacity &gt; elementData.length &amp;&amp; !(elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA &amp;&amp; minCapacity &lt;= DEFAULT_CAPACITY)) { modCount++; grow(minCapacity); }}//-------------------------- 以下来自ArraysSupport --------------------------public static int newLength(int oldLength, int minGrowth, int prefGrowth) { // assert oldLength &gt;= 0 // assert minGrowth &gt; 0 int newLength = Math.max(minGrowth, prefGrowth) + oldLength; if (newLength - MAX_ARRAY_LENGTH &lt;= 0) { return newLength; } return hugeLength(oldLength, minGrowth);}private static int hugeLength(int oldLength, int minGrowth) { int minLength = oldLength + minGrowth; if (minLength &lt; 0) { // overflow throw new OutOfMemoryError(\"Required array length too large\"); } if (minLength &lt;= MAX_ARRAY_LENGTH) { return MAX_ARRAY_LENGTH; } return Integer.MAX_VALUE;} 首次调用add方法添加元素或初始化后调用ensureCapacity​扩容时，走以上代码第9行的逻辑，扩容至少​DEFAULT_CAPACITY​即10个元素的空间长度； 当原来的数组有值但空间不足或手动调用ensureCapacity​扩容时，计算扩容空间的逻辑： 最少扩容空间(minGrowth)为传入参数与已有容量之差(大于0)，建议容量(prefGrowth)为已用用量的一半，MAX_ARRAY_LENGTH为整型最大值减8（减8的原因是某些VM在数组中保留一些头部信息） 在扩容后的长度小于MAX_ARRAY_LENGTH的情况下，取最小扩容容量和建议容量值之间的最大者；也就是说单次扩容容量起码是已分配空间的一半 当扩容后的长度大于MAX_ARRAY_LENGTH时，可能是由于增长了prefGrowth​导致的，则改为按​minGrowth​重新计算扩容后的长度： 如果扩容后的长度已经整型溢出，则抛异常 如果扩容后的长度小于等于MAX_ARRAY_LENGTH，则返回MAX_ARRAY_LENGTH 否则返回Integer.MAX_VALUE​ batchRemove分析 removeAll​和​retainAll都使用了​batchRemove​方法来处理，其代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243public boolean removeAll(Collection&lt;?&gt; c) { return batchRemove(c, false, 0, size);}public boolean retainAll(Collection&lt;?&gt; c) { return batchRemove(c, true, 0, size);}boolean batchRemove(Collection&lt;?&gt; c, boolean complement, final int from, final int end) { Objects.requireNonNull(c); final Object[] es = elementData; int r; // Optimize for initial run of survivors for (r = from;; r++) { if (r == end) return false; if (c.contains(es[r]) != complement) break; } int w = r++; try { for (Object e; r &lt; end; r++) if (c.contains(e = es[r]) == complement) es[w++] = e; } catch (Throwable ex) { // Preserve behavioral compatibility with AbstractCollection, // even if c.contains() throws. System.arraycopy(es, r, es, w, end - r); w += end - r; throw ex; } finally { modCount += end - w; shiftTailOverGap(es, w, end); } return true;}private void shiftTailOverGap(Object[] es, int lo, int hi) { System.arraycopy(es, hi, es, lo, size - hi); for (int to = size, i = (size -= hi - lo); i &lt; to; i++) es[i] = null;} complement​是​补数、补充、辅助的意思，其为true时计算的是与集合c的并集，其为false时计算的是与集合c的差集，这里使用了双指针的技巧来计算差集并集： 第13行定义一个下标指针r（可理解为read），接下来初始化为参数的from即数组第一个元素下标，它将负责从头到尾遍历对象数组； 第15~20行，找到第一个不在c（求并集）或第一个在c（求差集）中出现的元素记录其下标到w，如果指针r最终指向了数组末尾，则说明c是数组的子集（求并集）或c与数组不存在交集，直接返回false； 第21~25行，继续指针r遍历对象数组的剩余部分，对于不在c（求并集）或在c（求差集）中出现的元素将其拷贝到w指向的位置然后将w往右移动一个元素，直到剩余元素遍历完成； 第26~31行，兼容处理c.contains​可能抛出异常的情况； 第33~34行，修改本ArrayList实例的modCount，并且置空w往后的元素空间为null 简单总结 ArrayList&lt;E&gt;​没有加锁同步，它不是线程安全的； 使用默认构造函数新建一个ArrayList&lt;E&gt;​对象在未使用之前它是空的，直到调用​ensureCapacity​或者​add方法时才去扩容数组空间，至少为10个元素容量大小； 已分配过空间再进行扩容的机制：尝试扩容为已有容量的1.5倍，如果扩容超出整型最大值的情况下（可能是按已有容量一半进行扩容的）重新判断，尝试按最小扩容量进行扩容，如果还是超出整型最大值则抛异常，否则如果没超出MAX_ARRAY_LENGTH则返回MAX_ARRAY_LENGTH，否则返回整型最大值; 如果可预见后续会使用比较大的空间，则可以先调用ensureCapacity​做一次分配，以减少自动频繁扩容带来的开销； 求与另一个集合的差集、并集时采用了双指针的技巧。","link":"/2016/05/01/4e7557d65574406c.html"},{"title":"观日落：白云山晚望","text":"2015年10月17日，第一次来爬广州白云山，天气很不错，戴上耳塞听着朴树的那首召唤，伴着晚风，45度角仰望爽朗的天空，且听风吟～ 是夜吗？是远方，是那阵忧愁我的晚风，在那往事翻动的夜，在儿时没能数清的星斗下～（朴树：《召唤》）","link":"/2015/10/18/d2565847705d4c7f.html"},{"title":"Ubuntu Server安装GUI环境","text":"Ubuntu Desktop默认是带XWindow图形界面程序的，而Ubuntu Server默认不带，需要自己安装，我是在安装了Ubuntu Server 14.04LTS之后再安装的XWindow，桌面环境使用的是GNome，你也可以使用KDE等其他的。比较搞的是这边不是使用有线网络，全部使用WIFI，而我的PC装的是Ubuntu Server，所以先要设置一下使用WIFI上网，然后安装XWindow和GNome，最后设置一下默认启用UI模式还是TEXT字符模式，安装桌面环境会自带安装上Firfox浏览器，由于系统是英文版，所以出现了网页中文乱码问题，安装一个中文字体即可。 设置Ubuntu使用WIFI上网需要安装wpasupplicant，但默认地Ubuntu Server已经自带了该软件，所以直接使用（不然又上不了网，可真的要蛋疼了）。不过还是提一下安装命令： 1$ sudo apt-get install wpasupplicant 将WIFI的SSID（就是你在Windows或者手机下点击无线所看到的那个WIFI名称）和密码写入配置文件，配置文件名和路径自己任意取： 1$ sudo wpa_passphrase WIFI_SSID WIFI_PASWD &gt; /etc/wifi_SSID.conf 编辑/etc/network/interfaces配置文件将WIFI无线配置信息写入其中： 1234$ sudo vim /etc/network/interfacesauto wlan0iface wlan0 inet dhcpwpa-conf /etc/wifi_SSID.conf 重启使配置生效（不过应该可以有其他方式使其生效而不用重启~ 如果你知道可以留言告知一下）： 1$ sudo reboot 安装XWindow和Gnome桌面环境安装XWindow： 12$ sudo apt-get install xserver-xorg$ sudo apt-get install x-window-system-core 使XWindow生效： 1$ sudo dpkg-reconfigure xserver-xorg 安装GNome桌面环境： 1$ sudo apt-get install gnome-core gdm 安装屏保程序： 1$ sudo apt-get install xscreensaver 安装相关显示字体： 1$ sudo apt-get install ttf-arphic* 解决Firfox中文乱码问题 原因： 缺少显示中文的字体（参考：AskUbuntu | Install chinese font），下载字体xfonts-wqy来安装就好了，命令如下： 1$ sudo apt-get install xfonts-wqy 解决了Firfox的乱码之后又出现了新的问题，那就是Firfox经常崩溃，于是安装了Google Chrome，问题是Chrome也是乱码，执行下面的命令安装wqy中文字体，搞定： 12$ sudo apt-get install ttf-wqy-*$ sudo apt-get install fonts-wqy-* 重装过一次系统，依据以前的经验，这次在安装gnome桌面系统之后我直接将字体给安装上了，后来再也没发现firfox有乱码： 1234$ sudo apt-get install ttf-arphic*$ sudo apt-get install ttf-wqy-*$ sudo apt-get install xfonts-wqy$ sudo apt-get install fonts-wqy-* 设置默认进入UI桌面环境1$ cat /etc/X11/default-display-manager 查看输出值，如果为/usr/sbin/gdm或类似则为默认进入图形界面，这里安装完成桌面环境之后其值就默认修改成了/usr/sbin/gdm；如果为false则默认进入TEXT字符命令界面。也可以在进入字符命令界面之后使用如下命令来启动桌面环境： 1$ startx 如果是从第一个TTY如TTY1，则执行之后对应在Ctrl+Alt+F7桌面环境，如果你再从第二个TTY启动UI环境则对应在Ctrl+Alt+F8，依次类推。 附加：切换各个TTY切换各个TTY的命令为： 123456TTY1：Ctrl+Alt+F1TTY2：Ctrl+Alt+F2TTY3：Ctrl+Alt+F3TTY4：Ctrl+Alt+F4TTY5：Ctrl+Alt+F5TTY6：Ctrl+Alt+F6","link":"/2015/11/15/aa3536157254b0cbb36.html"},{"title":"Linux设置和使用本地源","text":"在虚拟机中安装RHEL做实验，安装时没有选择安装任何服务器组件，打算在安装完成基本系统之后自己再装的，但在进入系统安装软件时发现提示有依赖无法安装。后来通过建立本地源的方式来解决了部分问题，说是解决部分问题是因为需要的软件包可能在安装包中就有，没有的就没办法解决了。制作本地源的方法是将ISO安装镜像拷贝到系统内，然后建立索引文件。 建立本地源其中的Packages文件夹中就存储着该镜像所包含的所有RPM软件包，需要用它来建立本地源。如在/挂载点下建立文件夹LocalRepo，将整个镜像里面的文件拷贝到RHEL中： 建库索引文件在RHEL中打开文件夹/etc/yum.repos.d，在其下建立一个*.repo文件如public-yum-ol6.repo，文件的内容如下： 123456[ol6_latest]name=Oracle_Linux_6baseurl=file:///LocalRepogpgkey=file:///LocalRepo/RPM-GPG-KEY-redhat-releasegpgcheck=1enabled=1 其中enable指明该源是否可用。由于源是在本地的/LocalRepo，所以baseurl写为baseurl=file: ///LocalRepo，gpgkey是一个源标识文件，该文件可以在ISO镜像中找到，不同版本的RHEL ISO镜像其gpgkey file可能不同，需要打开ISO查看，从上图可用看出该ISO镜像中的gpgkey file为RPM-GPG-KEY-redhat-release，所以gpgkey为：gpgkey=file:///LocalRepo/RPM- GPG-KEY-redhat-release。编写好repo文件之后保存，这样就可以使用本地源了。 使用本地源使用本地源即与一般的使用yum命令一样，只是建立本地源之后yum会自动在本地源中搜索。使用-Uvh命令可以解决依赖问题（yum自动搜索并解决依赖）。更好的方法是找到rpm包之后直接双击安装，这样也会自动搜索解决依赖。 1$ rpm -Uvh *.rmp","link":"/2015/11/02/6420f8324eea4d218.html"},{"title":"使用VSFTP搭建FTP服务器","text":"官方在2015年5月26日发布了Fedora 22，这里基于该版本的服务器和VSFTPD，记录如何安装配置FTP服务器。 安装VSFTP服务器版本：Fedora Server 22。安装vsftpd，使用以下命令(Fedora22，之前的请使用yum命令替代dnf命令) 1$ sudo dnf install vsftpd 配置文件在/etc/vsftpd/目录下，名为vsftpd.conf，服务器的根目录在/var/ftp下，其中匿名账户的目录为子目录/var/ftp/pub，为了简便直接修改配置使用匿名账户来上传下载文件，开启下列配置项： 1234anonymous_enable=YESwrite_enable=YESanon_upload_enable=YESanon_mkdir_write_enable=YES 可能需要关闭防火墙才可以访问。Fedora 17及以前的版本关闭防火墙用命令： 1$ sudo systemctl stop iptables.service Fedora 18及上名字改为了firewalld，即： 1$ sudo systemctl stop firewalld.service 如果想禁止该服务项自启动可执行以下命令来关闭服务： 1$ sudo systemctl disable firewalld.service 配置VSFTP查看文件/etc/passwd中ftp一项的值ftp:x:14:50:FTP User:/var/ftp:/sbin/nologin，可以看出ftp的跟目录/var/ftp禁止ftp这个用户从控制台登陆，即只能从ftp访问。对于匿名ftp用户，为了让其能够上传下载文件，需要修改/var/ftp和/var/ftp/pub目录的相应权限：/var/ftp/pub需要修改为744，归属和组别都要设置为ftp，否则在CWD该目录时会报错Response: 550 Failed to change directory. Error: Failed to retrieve directory listing，而/var/ftp则使用755，用户和归属为root。综上做如下设置： 12345$ cd /var/$ sudo chown -R root:root ftp$ sudo chmod -R 755 /var/ftp$ sudo chown -R ftp:ftp ftp/pub$ sudo chmod -R 744 ftp/pub 此外需要关闭SELINUX，否则无法上传文件，会报错误：553 Could not create file. Error: Critical file transfer error，将SELINUX值由默认的enforcing改为permissive，然后重启： 12345[admin@svr] cd /etc/selinux[admin@svr] sudo vim config#SELINUX=enforcingSELINUX=permissive[admin@svr]$ 不想重启机器的情况下也可以执行命令setenforce 0。命令setenforce 1设置SELinux 成为enforcing模式；setenforce 0设置SELinux 成为permissive模式。 参考文档 vsftp | docs vsftp详细配置、/etc/vsftpd/vsftpd.conf CentOS下vsftp设置、匿名用户&amp;本地用户设置、PORT、PASV模式设置","link":"/2015/10/02/7607fe3b0f8b4815.html"},{"title":"开启RSA认证使用SSH免密登录","text":"通过shell登录远端Linux服务器有两种基本方式，一种是使用用户名密码，一种是使用SSH公私钥认证，这里记录如何开启Linux系统下的SSH RSA登录认证。 开启RSA认证远程服务器上SSH的配置文件/etc/ssh/sshd_config指定了是否开启RSA认证，修改该配置文件需要root权限。开启RSA认证要保证该配置文件中的如下选项被开启： 123RSAAuthentication yesPubkeyAuthentication yesAuthorizedKeysFile .ssh/authorized_keys 然后在用户的home目录下创建文件夹.ssh并在该文件夹里面authorized_keys文件，设置权限： 123$ cd$ sudo chmod 700 .ssh$ sudo chmod 600 .ssh/authorized_keys 创建SSH Key这里举例类Unix客户端环境，例如我的是Macbook Pro： 12$ ssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"Generating public/private rsa key pair. 以上命令用于使用名称ID “your_email@example.com“ 来生成公钥/私钥对，如下指定生成的私钥存放地址和私钥密码： 123Enter a file in which to save the key (/Users/you/.ssh/id_rsa): [回车]Enter passphrase (empty for no passphrase): [输入密码]Enter same passphrase again: [再次输入密码] 启动ssh-agent然后添加私钥到ssh-agent ： 123$ eval \"(ssh-agent -s)\"Agent pid 2153$ ssh-add ~/.ssh/id_rsa 添加公钥到服务器将~/.ssh/id_rsa.pub公钥的内容拷贝到剪贴板（Mac下可以使用pbcopy命令）： 12$ cd ~/.ssh$ pbcopy &lt; id_rsa.pub 然后粘贴到服务器上的.ssh/authorized_keys文件中，在客户端就可以通过如下命令进行登录了（不用再输入密码）： 1$ ssh user@server_domain","link":"/2015/11/03/1c2df78899b349c5b.html"},{"title":"用OpenSSL颁发自签名证书","text":"自己生成证书可以用于构建证书颁发服务来为客户端授信，这里记录如何使用openssl生成证书并在Nginx中配置和使用。 自签名证书生成一个RSA私钥 ： 1$ openssl genrsa -des3 -out fvpnd.key 1024 拷贝一个不需要输入密码的密钥文件 1$ openssl rsa -in fvpnd.key -out fvpnd_nopass.key 生成一个证书请求 1$ openssl req -new -key fvpnd.key -out fvpnd.csr 签发证书，有效期365天 1$ openssl x509 -req -days 365 -in fvpnd.csr -signkey fvpnd.key -out fvpnd.crt Nginx配置和使用将方才生成的证书拷贝到/etc/nginx/ssl/；安装好Nginx之后修改/usr/local/nginx下的nginx.conf，注释掉server这一节，加入一句指令来另外指定各个站点的配置文件路径： 12#gzip on;include /usr/local/nginx/conf/vhosts/*.conf; 新建 /usr/local/nginx/conf/vhosts 目录，新建fvpnd.conf文件，内容： 123456789101112131415161718# HTTPS serverserver { #listen 80; listen 443 ssl; server_name localhost; server_name 127.0.0.1; ssl_certificate /etc/nginx/ssl/fvpnd.crt; ssl_certificate_key /etc/nginx/ssl/fvpnd_nopass.key; root /var/app/nginx/vhost/fvpnd; index index.html index.htm; location / { # for uWSGI uwsgi_pass 127.0.0.1:53020; include uwsgi_params; }} 免费证书如StartSSL，但证书有效期只有1年。","link":"/2015/11/03/5ffb348b1e9344848.html"},{"title":"在CentOS集群上部署DNS服务","text":"在Kubernetes集群搭建中记录了构建6台机器的集群用以跑K8s的搭建过程，本文记录在集群内搭建一个DNS服务、让各个机器在集群内可通过域名可访问其他机器的过程。集群机器的IP、Hostname等的设计如下： Hostname IP Address 备注说明 worker-st.xcluster.io 192.168.99.100 DNS Master worker-01.xcluster.io 192.168.99.101 作为K8s集群的master节点，DNS Slave worker-02.xcluster.io 192.168.99.102 作为K8s集群的1号Node机器，负责跑Pods worker-03.xcluster.io 192.168.99.103 作为K8s集群的2号Node机器，负责跑Pods worker-04.xcluster.io 192.168.99.104 作为K8s集群的3号Node机器，负责跑Pods worker-05.xcluster.io 192.168.99.105 作为K8s集群的4号Node机器，负责跑Pods 选择192.168.99.100和192.168.99.101作为DNS服务器的主DNS和DNS Slave。 安装DNS Server在两台机器上都执行相同的安装操作来安装named服务： 1$ sudo yum install bind bind-utils -y 安装完成以后通过如下命令设置为系统自启动服务，启动|停止|重启服务，及查看服务状态： 123$ sudo systemctl enable named$ sudo systemctl start|stop|restart named$ sudo systemctl status named DNS服务的配置文件为/etc/named.conf。 配置DNS Server配置Master编辑配置文件/etc/named.conf，在listen-on当中添加主DNS的IP地址（本机IP：192.168.99.100）、添加allow-query和allow-transfer选项，并添加我们即将配置的xcluster.io域名的zone配置文件路径；添加后如下： 12345678options { listen-on port 53 { 127.0.0.1; 192.168.99.100;}; # 省略其他配置 allow-query { localhost; 192.168.99.0/24;}; ### 子网范围 ### allow-transfer { localhost; 192.168.99.101; }; ### DNS Slave的IP地址 ### # 省略其他配置 include \"/etc/named/xcluster.io.zones\";} 然后新建/etc/named/xcluster.io.zones文件，为xcluster.io指定zone配置信息： 1234567891011zone \"xcluster.io\" IN { type master; file \"forward.xcluster\"; allow-update { none; };};zone \"99.168.192.in-addr.arpa\" IN { # 注意到命名开始是IP地址里面的网络号的反写 type master; file \"reverse.xcluster\"; allow-update { none; };}; 我们在zone文件中指定了具体的配置文件，接下来我们将创建和配置这两个文件，他们要放置在/var/named目录下： /var/named/forward.xcluster.io： 123456789101112131415161718192021$TTL 86400@ IN SOA masterdns.xcluster.io. root.xcluster.io. ( 2021010401 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL)@ IN NS masterdns.xcluster.io.@ IN NS secondarydns.xcluster.io.@ IN A 192.168.99.100@ IN A 192.168.99.101masterdns IN A 192.168.99.100secondarydns IN A 192.168.99.101worker-st.xcluster.io IN A 192.168.99.100worker-01.xcluster.io IN A 192.168.99.101worker-02.xcluster.io IN A 192.168.99.102worker-03.xcluster.io IN A 192.168.99.103worker-04.xcluster.io IN A 192.168.99.104worker-05.xcluster.io IN A 192.168.99.105 其中16~21行是我们指定的A记录配置。 /var/named/reverse.xcluster.io： 12345678910111213141516171819202122$TTL 86400@ IN SOA masterdns.xcluster.io. root.xcluster.io. ( 2021010401 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL)@ IN NS masterdns.xcluster.io.@ IN NS secondarydns.xcluster.io.@ IN PTR xcluster.io.100 IN PTR masterdns.xcluster.io.101 IN PTR secondarydns.xcluster.io.masterdns IN A 192.168.99.100secondarydns IN A 192.168.99.101100 IN PTR worker-st.xcluster.io # 注意到第一列是IP地址里面的地址号101 IN PTR worker-01.xcluster.io102 IN PTR worker-02.xcluster.io103 IN PTR worker-03.xcluster.io104 IN PTR worker-04.xcluster.io105 IN PTR worker-05.xcluster.io 其中17~22行是我们指定的反向记录配置。 配置Slave类似Master的配置，编辑配置文件/etc/named.conf，在listen-on当中添加DNS Slave的IP地址（本机IP：192.168.99.101）、添加allow-query（这里没有了allow-transfer）选项，并添加我们即将配置的xcluster.io域名的zone配置文件路径；添加后如下： 12345options { listen-on port 53 { 127.0.0.1; 192.168.99.100;}; # 省略其他配置 include \"/etc/named/xcluster.io.zones\";} 注意，/etc/named/xcluster.io.zones的内容和master中的将有所不同： 12345678910zone \"xcluster.io\" IN { type slave; file \"slaves/forword.xcluster\"; masters { 192.168.99.100; };};zone \"99.168.192.in-addr.arpa\" IN { type slave; file \"slaves/reverse.xcluster\"; masters { 192.168.99.100; };}; 配置完以后，Slave将根据该配置自动从Master中同步xcluster.io的zones配置，同步过来的配置存放在/var/named/slaves文件夹下。 配置检查通过named-checkconf命令检查conf文件配置是否合法（合法时无输出），通过named-checkzone命令检查zone配置是否合法（合法时输出如下）： 1234$ sudo named-checkconf /etc/named.conf$ sudo named-checkzone xcluster.io /var/named/forward.xclusterzone xcluster.io/IN: loaded serial 2021010401OK 防火墙和SELinux设置如果Master、Slave启用了防火墙和SELinux，则需要做如下配置： 12345$ sudo firewall-cmd --permanent --add-port=53/tcp$ sudo firewall-cmd --reload$ sudo chgrp named -R /var/named$ sudo restorecon -rv /var/named 修改配置使用该DNS服务在集群设置中，我们使用了两张网卡(eth0和eth1)，其中eth0是DHCP方式分配IP，eth1是静态分配，在集群各个机器的/etc/sysconfig/network-scripts目录下修改ifcfg-eth0和ifcfg-eth1两个网卡的内容，指定DNS： 123PEERDNS=\"no\"DNS1=\"192.168.99.100\"DNS2=\"192.168.99.101\" 重启网络： 1$ sudo systemctl restart network 查看/etc/resolv.conf，发现已经被修改为使用了我们配置的DNS服务： 1234# Generated by NetworkManagersearch xcluster.ionameserver 192.168.99.100nameserver 192.168.99.101 在master、salve上通过dig命令查询配置的DNS记录信息： 1234567891011121314151617181920212223242526272829[root@Worker-01 network-scripts]# dig masterdns.xcluster.io; &lt;&lt;&gt;&gt; DiG 9.11.4-P2-RedHat-9.11.4-26.P2.el7_9.3 &lt;&lt;&gt;&gt; masterdns.xcluster.io;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 62978;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 2, ADDITIONAL: 2;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;masterdns.xcluster.io. IN A;; ANSWER SECTION:masterdns.xcluster.io. 86400 IN A 192.168.99.100;; AUTHORITY SECTION:xcluster.io. 86400 IN NS secondarydns.xcluster.io.xcluster.io. 86400 IN NS masterdns.xcluster.io.;; ADDITIONAL SECTION:secondarydns.xcluster.io. 86400 IN A 192.168.99.101;; Query time: 1 msec;; SERVER: 192.168.99.100#53(192.168.99.100);; WHEN: Mon Jan 04 14:27:48 CST 2021;; MSG SIZE rcvd: 123[root@Worker-01 network-scripts]# 可以看到18、19行就是我们刚配置的nameserver，PING一个域名看看结果： 1234567891011[root@Worker-01 network-scripts]# ping worker-st.xcluster.ioPING worker-st.xcluster.io.xcluster.io (192.168.99.100) 56(84) bytes of data.64 bytes from worker-st.xcluster.io.99.168.192.in-addr.arpa (192.168.99.100): icmp_seq=1 ttl=64 time=0.274 ms64 bytes from worker-st.xcluster.io.99.168.192.in-addr.arpa (192.168.99.100): icmp_seq=2 ttl=64 time=0.575 ms64 bytes from worker-st.xcluster.io.99.168.192.in-addr.arpa (192.168.99.100): icmp_seq=3 ttl=64 time=0.294 ms64 bytes from worker-st.xcluster.io.99.168.192.in-addr.arpa (192.168.99.100): icmp_seq=4 ttl=64 time=0.370 ms^C--- worker-st.xcluster.io.xcluster.io ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 3005msrtt min/avg/max/mdev = 0.274/0.378/0.575/0.119 ms[root@Worker-01 network-scripts]# 从结果可以看到域名是从worker-st.xcluster.io.99.168.192.in-addr.arpa解析出来的，域名服务器搭建完成。 参考文档 https://www.unixmen.com/setting-dns-server-centos-7/ https://help.dyn.com/how-to-format-a-zone-file/ https://en.wikipedia.org/wiki/Domain_Name_System","link":"/2020/12/26/9bf8861aca85e8311.html"},{"title":"基于Hexo搭建博客","text":"以前租用服务器自己买域名，自己搭建系统构建环境来玩Wordpress博客，要每个月交钱还要经常备份；最近决定抛弃Wordpress拥抱github pages，使用Hexo来构建博客然后放到Github Page上，这样就不用考虑域名备案和租用服务器，成本低到只需要一个域名。 安装Node.js安装Git (Git Doc)，若安装了Xcode则默认会安装Git for Mac。然后下载安装Node.js引擎，很简单。 安装Hexo安装好了Node.js就可以使用npm命令安装Hexo 1$ npm install hexo-cli -g 很可能会出现权限问题导致安装失败，可使用sudo重试 1$ sudo npm install hexo-cli -g 配置博客目录任意一个文件夹作为blog工作区，如命名为blogws，然后初始化该文件夹为博客工作区，如果更换了文件夹，同样要执行以下操作来初始化工作区。注意：应该尽量避免使用sudo来做接下来的一些列操作，因为使用sudo意味着文件权限归属是超级用户而不是当前用户，这在后面可能会引起一些问题，例如远程部署到github上的时候，就不能使用超级用户也就是sudo来部署，这会被github服务器拒绝，参见链接。 123$ mkdir blogws$ cd blogws$ hexo init 根据提示安装其他依赖： 1$ npm install 这样一个基本的博客工作目录就完成了，但这样还远远不够。事实上Hexo只是提供了一个平台，我们还需要安装其他的Hexo插件来完成诸如静态页面生成、本地预览、远程部署(例如部署到Git服务器)等，关于Hexo的可用插件可以在这个链接当中找到。Hexo及其插件是基于Node.js来开发的，安装的命令一般为 1$ npm install &lt;hexo-plugin-name&gt; --save 安装完成后相应的组件就会安装到博客目录中的node_modules文件夹下。针对我自己的情况，我安装了如下组件： 12345678910111213141516$ npm install hexo --save$ npm install hexo-search$ npm install hexo-deployer-git --save$ npm install hexo-generator-archive --save$ npm install hexo-generator-category --save$ npm install hexo-generator-feed --save$ npm install hexo-generator-index --save$ npm install hexo-generator-sitemap --save$ npm install hexo-generator-tag --save$ npm install hexo-renderer-ejs --save$ npm install hexo-renderer-marked --save$ npm install hexo-renderer-stylus --save$ npm install hexo-ruby-character --save$ npm install hexo-server --save$ npm install hexo-tag-bootstrap --save$ npm install hexo-uglify --save # compress js 其中server用来在本地预览，deployer-git用来部署到git服务器，如果你需要部署到其他类型的服务器，就需要安装其他组件如Heroku、OpenShift、Rsync、Amazon S3等等，参考Hexo Plugin来下载安装相应组件。对于安装静态页面生成器，如果想偷懒就直接把generator全部安装了，如 12$ npm install hexo --save$ npm install hexo-generator-* --save 撰写发布在source/_post下撰写博客文章(*.md文本文件)，撰写文章需要学一下Markdown标记语法。在blogws文件夹下使用命令在本地预览，如果成功(看提示)则可以通过地址http://0.0.0.0:4000/(地址和端口可在blogws下的_config.yml文件中配置)来在本地预览博客(需要安装hexo-server) 1$ hexo server # or hexo s 在blogws文件夹下使用命令生成静态页面(需要安装hexo-generator-*) 1$ hexo generate # or hexo g 使用下面的命令 1$ hexo deploy # or hexo d 来将静态页面发布到GitHub(需要配置ssh，参见如何在Github配置SSH，需要配置_config.yml)这里不要使用sudo，否则会部署失败，参见链接。发布需要会使用Git，Git类似svn，是用来做版本管理的，但它比svn更强大，Git使用参考。 参考资料 Git Docs Hexo Docs | 主题 About Markdown Markdown Basics 撰写工具：Vim or Sublime","link":"/2015/10/01/7544fcff345a42e1.html"},{"title":"Kubernetes集群搭建","text":"先期工作机器集群实现方案手头并没有现成的机器群，但好在有一台还算高配的MacBook Pro，如此的话可以在其上借助于虚拟机构建出一个5台机器的集群，在此基础上来搭建Kubernetes集群： 1234MacBook Pro (13-inch, 2017, Two Thunderbolt 3 ports)Processor: 2.5 GHz Dual-Core Intel Core i7Memory: 16 GB 2133 MHz LPDDR3Graphics: Intel Iris Plus Graphics 640 1536 MB 项目 选件 理由 虚拟机 VirtualBox 主要考虑Oracle Virtual Box和VMware Fusion，Vbox提供丰富的命令工具来操作虚拟机，这使得我们能够方便地基于此来编写一些控制脚本方便地控制集群机器，所以虚拟机选择VBox 操作系统 CentOS 7.9 主要考虑的是主流的RHEL、CentOS、Fedora、Ubuntu等等，RHEL/CentOS/Fedora同出一脉，Fedora作为试验先行版自然是不考虑了，CentOS比较主流，是首选，版本的话也是选择目前的主流版本7 Kubernetes集群设计计划使用6台虚拟机来构建集群，集群机器的IP、Hostname等的设计如下： Hostname IP Address 备注说明 worker-st.xcluster.io 192.168.99.100 不作为K8s集群的一部分，而是安装jenkins/gitlab….等CICD Devops工具 worker-01.xcluster.io 192.168.99.101 作为K8s集群的master节点，负责管理其他节点 worker-02.xcluster.io 192.168.99.102 作为K8s集群的1号Node机器，负责跑Pods worker-03.xcluster.io 192.168.99.103 作为K8s集群的2号Node机器，负责跑Pods worker-04.xcluster.io 192.168.99.104 作为K8s集群的3号Node机器，负责跑Pods worker-05.xcluster.io 192.168.99.105 作为K8s集群的4号Node机器，负责跑Pods 参考文档列表 在CentOS集群上部署DNS服务 https://kubernetes.io/zh/docs/home/ https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/install-kubeadm/ https://kubernetes.io/zh/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/ 创建服务器群创建CentOS7样机这一步的目的是创建出一台基础的CentOS7虚拟机并做好基础设置，然后基于该机器再克隆出其他机器，首先是创建第一台（worker-st.xcluster.io）并完成基本配置： 下载安装VBox：https://www.virtualbox.org/wiki/Downloads 下载CentOS7.9镜像：http://mirrors.aliyun.com/centos/7.9.2009/isos/x86_64/ 安装好VBox后创建一个虚拟机，配置如下： 名称：Worker-st、类型：Linux、版本：Red Hat (64bit) 内存：4096MB、CPU：双核、显存：64MB、显示器：2个、显示器显示比例：200% 磁盘类型：Normal (VMDK)、磁盘容量：300.00 GB(单个文件、动态增长) 网卡1：NAT、适配器类型：Paravirtualized Network (virtio-net) —— 用以跟MBP共享网络 网卡2：Host-only Adapter — — 用以设置静态IP（192.168.99.XX） 两张网卡的Promiscuous Mode均为默认值：Deny 在虚拟机磁盘中选择下载好的CentOS 7.9的ISO镜像，然后启动虚拟机进行系统安装，安装期间系统设置如下： 语言：English (United States)、键盘：English (United States) 磁盘分区：自动、Root密码：123456abc、创建管理账户：fury，密码为：123456abc 安装完成后以root登录，然后设置静态IP，这样可以直接从MBP的Terminal远程登录进机器，操作更方便一些（直接从虚拟机的黑框框登录的话，由于显示比例的问题，操作不是很方便）： 安装net-tools(包含ifconfig)：$ sudo yum install net-tools 启用SSH登录：添加MBP的公钥到~/.ssh/authorized_keys，重启SSHD，修改.ssh目录权限 123$ sudo service sshd restart$ sudo chmod 700 ~/.ssh$ sudo chmod 600 ~/.ssh/authorized_keys 设置静态IP 设置DNS $ sudo vi /etc/resolv.conf，加入nameserver 114.114.114.114 查看现有网卡：$ ifconfig -a 配置两张网卡（路径/etc/sysconfig/network-scripts/）： 备份enp0s3配置文件，并拷贝为ifcfg-eth0，内部改name和device为eth0（如果文件内没有HWADDR字段则加上，值为虚拟机网卡设置那里看到的MAC地址HWADDR=00:0C:29:7C:00:34），修改ONBOOT=yes 以自启动网卡 创建ifcfg-eth1文件，设置内容如下（配置第二张网卡为静态IP）： 1234567DEVICE=eth1HWADDR=00:0C:29:84:35:09ONBOOT=yesBOOTPROTO=staticIPADDR=192.168.99.100NETMASK=255.255.255.0NAME=eth1 HWADDR为虚拟机上第二张网卡的MAC地址 然后重启：$ sudo service network restart，可能会发现提示错误 查看$ sudo journalctl -xe或查看错误日志看看是哪个网卡没起来：$ cat /var/log/messages | grep network，发现说eth0没起来，它的UUID和eth1冲突了，把eth0的UUID删掉（确认HWADDR是第一张网卡的MAC地址），然后重启即可（UUID会自动加上）。 重启系统，这样以后就可以通过MBP的Terminal如下命令开启、登录、关闭虚拟机（XXX为虚拟机名称）： 开启虚拟机：$ VBoxManage startvm XXX -type vrdp 登录虚拟机：$ ssh root@192.168.99.100 关闭虚拟机：$ VBoxManage controlvm XXX poweroff 其他的基础设置： 安装wget下载工具：$ sudo yum install wget 修改hostname：$ sudo vi /etc/hostname，设置为worker-st.xcluster.io 修改shell显示名：$ sudo vi ~/.bashrc添加export PS1='[\\u@Worker-st \\W]\\$ ' 使用阿里云的Repo（备份原/etc/yum.repos.d/下的repo文件然后清空该文件夹）： 12345$ cd /etc/yum.repos.d/$ wget http://mirrors.aliyun.com/repo/Centos-7.repo$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo$ wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo$ yum clean all &amp;&amp; yum makecache 安装Vim8.x： 安装所需的基础库： 1$ yum install -y gcc gcc-c++ ruby ruby-devel lua lua-devel ctags git python python-devel tcl-devel ncurses-devel perl perl-devel perl-ExtUtils-ParseXS perl-ExtUtils-CBuilder perl-ExtUtils-Embed 下载Vim-v8.1： 123$ wget https://github.com/vim/vim/archive/v8.1.1312.tar.gz$ tar -zxvf v8.1.1312.tar.gz$ cd vim-8.1.1312 开始安装：--with-python-config-dir这项要看自己的实际路径，是python-devel带的目录 1234567891011$ ./configure --with-features=huge \\ --enable-multibyte \\ --enable-rubyinterp=yes \\ --enable-pythoninterp=yes \\ --with-python-config-dir=/usr/lib64/python2.7/config \\ --enable-perlinterp=yes \\ --enable-luainterp=yes \\ --enable-cscope \\ --prefix=/usr/local$ make -j4 VIMRUNTIMEDIR=/usr/local/share/vim/vim81 all$ sudo sudo make -j4 install 更改系统默认编辑器并将vi指向vim 1234$ sudo update-alternatives --install /usr/bin/editor editor /usr/local/bin/vim 1$ sudo update-alternatives --set editor /usr/local/bin/vim$ sudo update-alternatives --install /usr/bin/vi vi /usr/local/bin/vim 1$ sudo update-alternatives --set vi /usr/local/bin/vim 安装其他基础软件： 12$ sudo yum install unzip zip$ sudo yum install gcc gcc-c++ glibc 至此，我们创建好了一台基础的CentOS7虚拟机，这台机器将作为集群设计里面的第一台worker-st.xcluster.io。 创建Kubernetes样机这一步在创建好的CentOS7虚拟机样机的基础上安装Kubernetes集群都需要安装的一些组件。从上一步创建好的虚拟机克隆出一台新的虚拟机，克隆时名称改为Worker-01并选择完全克隆方式。克隆完成后在虚拟机的网卡设置当中刷新两张网卡的MAC地址，然后启动虚拟机，进入虚拟机后做如下修改： 注意：后面基于这台机器克隆其他k8s集群机器时也要做如下这些方面对应的修改 修改eth0和eth1的MAC地址和eth1的IP地址（路径/etc/sysconfig/network-scripts/） eth0和eth1的MAC地址（HWADDR）可以从虚拟网卡设置界面获得 IP地址按照一开始的设计，我们把IP改为192.168.99.101 修改hostname为worker-01.xcluster.io 修改shell显示名：$ vi ~/.bashrc添加export PS1='[\\u@Worker-01 \\W]\\$ ' 其他为Kubernetes进行的配置： 关闭防火墙：$ sudo systemctl stop firewalld &amp;&amp; systemctl disable firewalld 永久关闭selinux：$ sudo vim /etc/selinux/config修改： 12#SELINUX=enforcingSELINUX=disabled 永久关闭swap：$ sudo vim /etc/fstab 注释最后一行（swap） 配置yum源(自带的kubernetes版本可能比较低)：安装方式（生产，kubeadmin） $ cd /etc/yum.repos.d/ $ vim kubernetes.repo 1234567[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg $ sudo yum clean all &amp;&amp; yum makecache 安装docker 12$ sudo yum install -y yum-utils device-mapper-persistent-data lvm2$ sudo yum -y install docker 由于内核可能不支持 overlay2所以需要升级内核或者禁用overlay2(选择禁用，安装完docker可以启动docker测试下是否支持，启动docker不报错的可以忽略这一步)：$ sudo vim /etc/sysconfig/docker修改--selinux-enabled=false 自启动docker服务：$ sudo systemctl start docker &amp;&amp; systemctl enable docker 设置服务器时区：$ sudo timedatectl set-timezone Asia/Shanghai 设置k8s相关参数： $ sudo vim /etc/sysctl.d/k8s.conf 12net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1 让其生效 12$ sudo sysctl -p$ sudo echo \"1\" &gt; /proc/sys/net/ipv4/ip_forward 安装k8s相关安装包，使用 $ sudo yum list kube* 来查看当前yum源支持的最新版本（这里是1.20.1-0） 1$ sudo yum -y install kubeadm-1.20.1-0 kubelet-1.20.1-0 kubectl-1.20.1-0 自启动服务：$ sudo systemctl restart kubelet &amp;&amp; systemctl enable kubelet 到这里完成了第二台样机设置，这台机器将作为k8s集群的master机器（worker-01.xcluster.io），其他机器将以这台为样本进行克隆配置。 克隆其他K8s集群机器基于第二台样机（worker-01.xcluster.io），克隆出其他的4台机器，注意按上面的提示对应修改MAC地址、IP、hostname、shell显示名等相关信息： Hostname IP Address 备注说明 worker-02.xcluster.io 192.168.99.102 集群1号Node机器 worker-03.xcluster.io 192.168.99.103 集群2号Node机器 worker-04.xcluster.io 192.168.99.104 集群3号Node机器 worker-05.xcluster.io 192.168.99.105 集群号Node机器 配置K8s集群到这里，我们创建好了6台CentoOS服务器，第一台后面作为安装Jenkins/gitlab等工具的机器，不纳入k8s集群，后面5台都安装了k8s相关包和docker，这5台将作为我们的k8s集群机器，这些机器的配置目前是一样的，接下来对第一台机器（worker-01.xcluster.io）进行设置（初始化k8s集群让其成为master）。 初始化配置Master在master机器上执行初始化操作，即worker-01.xcluster.io这台机；注意坑点： 如果要用flannel: 如果想配合使用dashboard，建议直接放弃，可改用weave，详细可以见这里 安装前,使用kubeadm init的时候，参数里记得指定service-cidr. 关于flannel和weave：https://www.cnblogs.com/kevingrace/p/6859114.html 如果要用weave: 安装前，使用kubeadm init的时候，参数里记得不要指定service-cidr参数，但要保留pod-network-cidr. 不然内置的DNS会出问题，这可能是系统的bug。 1234567891011121314151617181920212223242526272829$ sudo kubeadm init \\--apiserver-advertise-address=192.168.99.101 \\ # 写masterIP地址即可--image-repository registry.aliyuncs.com/google_containers \\--kubernetes-version v1.20.1 \\ # 注意是v1.20.1而不是v1.20.1-0--service-cidr=10.2.0.0/16 \\--pod-network-cidr=10.244.0.0/16Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.99.101:6443 --token 9k35nl.lyfuzdlxzqbwonsr \\ --discovery-token-ca-cert-hash sha256:91f6bd754f302b858b02d02f16e164d72b39a9ebcba841931e1e80388fd91910 [fury@Worker-01 ~]$ 可以看到初始化已经OK了，通过26行的代码可以在其他node上执行以让master发现（加入k8s集群）。 安装flannel： 下载https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml，然后 $ sudo kubectl apply -f kube-flannel.yml 到这里master就初始化完毕了。 其他节点加入K8s集群 将node节点加入到master集群中，在需要加入k8s集群的node节点上执行该命令（上面的输出） 12$ sudo kubeadm join 192.168.99.101:6443 --token 9k35nl.lyfuzdlxzqbwonsr \\ --discovery-token-ca-cert-hash sha256:91f6bd754f302b858b02d02f16e164d72b39a9ebcba841931e1e80388fd91910 如果忘记kubeadm join后的参数，可用通过下面命令来获取新的： 1$ sudo kubeadm token create --print-join-command 在Worker-02上的操作结果： 12345678910111213141516[fury@Worker-02 ~]$ sudo kubeadm join 192.168.99.101:6443 --token 9k35nl.lyfuzdlxzqbwonsr --discovery-token-ca-cert-hash sha256:91f6bd754f302b858b02d02f16e164d72b39a9ebcba841931e1e80388fd91910 [preflight] Running pre-flight checks[preflight] Reading configuration from the cluster...[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"[kubelet-start] Starting the kubelet[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...This node has joined the cluster:* Certificate signing request was sent to apiserver and a response was received.* The Kubelet was informed of the new secure connection details.Run 'kubectl get nodes' on the control-plane to see this node join the cluster.[fury@Worker-02 ~]$ 在其他的Worker上执行相同操作来加入集群，正常的话在master上执行``即可看到其他node的状态： 12345678[fury@Worker-01 ~]$ sudo kubectl get nodesNAME STATUS ROLES AGE VERSIONworker-01.xcluster.io Ready control-plane,master 2d22h v1.20.1worker-02.xcluster.io Ready &lt;none&gt; 2d22h v1.20.1worker-03.xcluster.io Ready &lt;none&gt; 2d17h v1.20.1worker-04.xcluster.io Ready &lt;none&gt; 2d17h v1.20.1worker-05.xcluster.io Ready &lt;none&gt; 2d17h v1.20.1[fury@Worker-01 ~]$ 常用的kubernetes命令 $ sudo systemctl start|restart|stop kubelet：开启、重启、停止kubelet $ sudo systemctl status -l kubelet：查看集群启动状态 $ sudo jornalctl -f -u kubelet：查看集群日志 $ sudo kubectl get &lt;node|nodes&gt;：列出所有node(及其状态) $ sudo kubectl delete node|nodes&gt; &lt;node name&gt;：删除集群节点 $ sudo kubectl get &lt;pod|pods&gt; [-n &lt;namespace&gt;]：列出所有的pods $ sudo kubectl get &lt;svc|service|services&gt; [-n &lt;namespace&gt;]：列出所有的服务 $ sudo kubectl apply -f &lt;deployment.yml&gt;：部署一个服务 $ sudo kubectl delete -f &lt;deployment.yml&gt;：移除一个服务 $ sudo kubectl logs -f &lt;pod name&gt; [-n &lt;namespace&gt;]：查看某个pod日志 安装配置过程中的问题 在master上执行$ sudo kubectl get nodes报错：The connection to the server localhost:8080 was refused - did you specify the right host or port? 解决： 12$ echo \"export KUBECONFIG=/etc/kubernetes/admin.conf\" &gt;&gt; ~/.bash_profile$ sudo source ~/.bash_profile 执行完$ sudo kubeadm join后在master节点执行$ sudokubectl get nodes 发现Node节点状态为NotReady； 在node节点查看日志$ sudo journalctl -f -u kubelet找到具体错误原因： summary_sys_containers.go:47] Failed to get system container stats for “/system.slice/kubelet.service”: failed to get cgroup stats for “/system.slice/kubelet.service”: failed to get container info for “/system.slice/kubelet.service”: unknown container “/system.slice/kubelet.service” 解决：是kubernetes和docker版本兼容性问题，启动时添加参数：**–runtime-cgroups=/systemd/system.slice –kubelet-cgroups=/systemd/system.slice**即可： 开机自启动的服务是通过$ sudo systemctl enable xxx来配置，该命令执行后会在/etc/systemd/system/multi-user.target.wants/目录下新建一个/usr/lib/systemd/system/xxx.service的文件并创建软连接到当前目录 在/usr/lib/systemd/system/xxx.service同目录下找到kubelet.service.d文件夹，有个/usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf文件，编辑，在KUBELET_CGROUP_ARGS属性添加： 1--runtime-cgroups=/systemd/system.slice --kubelet-cgroups=/systemd/system.slice 如果不存在”KUBELET_CGROUP_ARGS”属性则新增，同时在ExecStart=/usr/bin/kubelet ....后新增$KUBELET_CGROUP_ARGS 10-kubeadm.conf的路径网上提到的多为：*/etc/systemd/system/kubelet.service.d/10-kubeadm.conf*，本人的机器路径为：/usr/lib/systemd/system/kubelet.service.d 然后重启$ sudo systemctl daemon-reload &amp;&amp; systemctl restart kubelet即可解决。 执行$ sudo journalctl -f -u kubelet或者$ sudo systemctl status -l kubelet发现日志中存在错误或警告： Unable to read config path “/etc/kubernetes/manifests”: path does not exist, ignoring 解决：执行$ sudo mkdir -p /etc/kubernetes/manifests然后重启kubelet。 failed to sync configmap cache: timed out waiting for the condition 解决：新增（编辑）/var/lib/kubelet/config.yaml添加 12featureGates: CSIMigration: false 然后重启所有node。 failed to collect filesystem stats - rootDiskErr: could not stat 解决：不支持overlay2导致的，编辑docker配置 $ sudo vim /etc/sysconfig/docker修改--selinux-enabled=false 然后重启docker$ sudo systemctl restart docker MemoryAccounting not enabled for pid: 1215， CPUAccounting not enabled for pid: 1208 查看这两个进程：$ sudo ps -a 1208、# ps -a 1215： 12345678[fury@Worker-01 ~]$ sudo ps -a 1208 PID TTY STAT TIME COMMAND 1208 ? Ssl 0:12 /usr/bin/dockerd-current --add-runtime docker-runc=/usr/libexec/docker/docker-runc-current --default-runtime=d 8015 pts/0 R+ 0:00 ps -a 1208[fury@Worker-01 ~]$ sudo ps -a 1215 PID TTY STAT TIME COMMAND 1215 ? Ssl 0:32 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/ku 8079 pts/0 R+ 0:00 ps -a 1215 查看内存使用情况：$ free -h 12345[fury@Worker-01 ~]$ free -h total used free shared buff/cache availableMem: 3.7G 868M 2.0G 10M 840M 2.6GSwap: 0B 0B 0B[fury@Worker-01 ~]$ 并没有存在内存不够的情况。 解决：明确指定DefaultCPUAccounting和DefaultMemoryAccounting即可 123456$ sudo mkdir -p /etc/systemd/system.conf.d$ sudo vim /etc/systemd/system.conf.d/kubernetes-accounting.conf[Manager]DefaultCPUAccounting=yesDefaultMemoryAccounting=yes$ sudo systemctl daemon-reload &amp;&amp; systemctl restart kubelet 执行$ sudo kubectl get pods： No resources found in default namespace 解决：不指定命名空间时默认在default这个命名空间；通过**-n**参数指定，系统命名空间为kube-system： 1$ sudo kubectl get pods -n kube-system","link":"/2020/12/26/f586b25f145a49b6b.html"},{"title":"MySQL与Postgres的比较","text":"MySQL是一个广受欢迎和使用的RDBMS，它的名字来源于创始人Michael Widenius女儿的名字My. MySQL源码基于GNU GPL协议开源，该项目现在由Oracle公司维护。它是一个主要在关系数据库模型上工作的RDBMS (Relational Database Management System)，它使得数据库管理更容易、更灵活。 Postgre是一个对象-关系数据库管理系统(ORDBMS)，它是在加利福尼亚大学计算机科学系开发的，它开创了许多概念。Postgre是一个企业级的关系数据库，很容易安装和设置，它同时支持SQL和NoSQL。Postgre有一个很好的社区，很乐意在你使用PostgreSQL时遇到问题时为你提供支持。 MySQL发展史 MySQL最初由Swedish公司在1995开发，称之为MySQL AB Sun公司在2008年以10亿美元收购MySQL AB Oracle在2010年全资收购Sun，MySQL也随之归到Oracle门下 2012年，Monty Program Ab创始人同时也是MySQL创始人Michael Widenius维护了一个新的MySQL分支即是后来的MariaDB MariaDB在2013年取代了大多数发行版的MySQL 2013年Monty Program Ab与SkySQL合并 2014年SkySQL Ab更名为MariaDB Corporation PostgreSQL发展史 1977年INGRES被开发出来 1986年Michael Stonebraker和他的同事开发了Postgres 1990年Postgres支持真正的ACID和PL/pgSQL 1995年发布Postgres95 1996年重新发布Postgres95为Postgres 6.0 1998~2001年，添加了MVCC，GUC，联接语法控件和过程语言加载器 2002~2006年，7.2版至8.2版：包含了新的功能如Schema支持，非阻塞VACUUM，角色(Roles)和dblink 2009年，发布Postgres 8.4 2010年，发布Postgres 9.0 2013年，NYCPUG（纽约市PostgreSQL用户组）加入PgUS（美国PostgreSQL协会） 2014年，PGconf组织成立 MySQL对比PostgreSQLMySQL与PostgreSQL的关键不同 PostgreSQL是一个对象关系数据库管理系统（ORDBMS），而MySQL是一个社区驱动的DBMS系统 PostgreSQL支持JSON，XML等现代应用程序功能，而MySQL仅支持JSON 执行复杂查询时，PostgreSQL的性能很好，而MySQL在OLAP和OLTP系统中的性能很好 PostgreSQL完全符合ACID，而MySQL仅与InnoDB和NDB一起使用时才符合ACID PostgreSQL支持物化视图（Materialized Views），而MySQL不支持物化视图（Materialized Views） 为何选择MySQL?选择使用MySQL的一些主要理由： 支持主从复制(Master-Slave Replication)，横向扩展(Scale-Out)等功能 它支持卸载报告(Offload Reporting)，地理数据分发(Geographic Data Distribution)等 当用于大多数场景只读的应用程序时，MyISAM存储引擎的开销非常低 支持常用表的内存存储引擎 重复使用的语句支持查询缓存 文档丰富、社区活跃，可从博客、白皮书和书籍等不同来源轻松学习MySQL并对其进行故障排除 为何选择PostgreSQL?选择使用Postgre的一些主要理由： 提供很有用的功能，例如表分区，时间点恢复，事务性DDL等 能够在完整的PKI基础架构中利用第三方密钥库 开发人员可以修改BSD许可的开放源代码，而无需提供任何回馈增强功能 独立软件供应商可以重新分发它，而不必担心被开源许可证“感染” 可以为用户和角色分配对象级别特权 支持AES，3DES和其他数据加密算法 MySQL的功能 MySQL是社区驱动的DBMS系统 与使用所有主要语言和中间件的各种平台兼容 它支持多版本并发控制 符合ANSI SQL标准 允许基于日志和基于触发器的复制SSL 面向对象且与ANSI-SQL2008兼容 具有独立模块的多层设计 完全多线程，使用内核线程 可以用于嵌入式数据库或客户端服务器模型 提供用于查询分析和空间分析的内置工具 它可以处理任何数量的数据，多达5000万行或更多 MySQL可在许多种类的UNIX以及Windows和OS / 2等其他非UNIX系统上运行 PostgreSQL的功能 拥有一个活跃的社区，正在加速其发展 是Oracle，DB2和SQL Server的最常见替代方案 可在所有主要OS平台上运行 MVCC支持大量并发用户 广泛的索引以实现高性能报告 支持现代应用程序（XML和JSON） 对可移植技能/代码的ANSI SQL支持 外键支持有效存储数据 表联接和视图可进行灵活的数据检索 复杂程序和交易的触发器/存储过程 复制以进行数据备份和读取可扩展性 MySQL与PostgreSQL区别 对比项 MYSQL PostgreSQL 开源许可 GNU GPL开源许可 类似BSD &amp; MIT的PostgreSQL许可 符合ACID 仅在InnoDB和NDB中支持ACID 完全支持ACID SQL兼容 部分兼容SQL(如不支持check检查约束) 很大程度上符合SQL 社区支持 拥有大量的贡献者社区，他们主要关注于维护现有功能，偶尔会出现新功能 活跃社区会不断完善现有功能，而创新社区会努力确保它保持最先进的数据库。 定期发布新的尖端功能和安全性增强功能 性能 它主要用于需要数据库进行直接数据交易的基于Web的项目 它在读取和写入速度非常重要的大型系统中得到高度使用 最佳套装 当仅关心读取速度时，MySQL在OLAP＆OLTP系统中表现良好 执行复杂查询时，PostgreSQL性能良好 JSON支持 支持JSON数据类型，但不支持任何其他NoSQL功能 支持JSON和其他NoSQL功能，例如XML支持；它还允许索引JSON数据，以加快访问速度 物化视图支持 支持实例化视图和临时表 支持临时表，但不提供实例化视图 生态系统 MySQL具有一个动态的生态系统，其变体包括MariaDB，Percona，Galera等 Postgres的高端选择有限。 但是，它随着最新版本中引入的新功能而改变 默认值 可以在会话级别和语句级别覆盖默认值 默认值只能在系统级别更改 B树索引 适当时可以使用两个或多个B树索引 在运行时合并并评估的B树索引是动态转换的谓词 对象统计 相当好的对象统计 很好的对象统计 StackOverflow 相关问题数量 532K 89.3K 联合查询能力 有限的联合查询能力 很好的联合查询能力 GitHub Stars 3.34k 5.6k GitHubForks 1.6k 2.4k 使用该产品的知名公司 Airbnb, Uber, Twitter Netflix, Instagram, Groupon MySQL的缺点 与系统目录相关的事务不符合ACID 有时服务器崩溃可能会损坏系统目录 没有可插拔的身份验证模块，无法集中管理帐户 不支持角色，因此很难为许多用户维护特权 存储过程不可缓存 用于过程或触发器的表总是预先锁定的 PostgreSQL的缺点 当前的外部解决方案需要较高的学习曲线 没有针对主要版本的升级功能 数据需要导出或复制到新版本 升级过程中需要双重存储 索引不能用于直接返回查询结果 查询执行计划未缓存 批量加载操作可能会限制CPU 稀疏独立软件供应商支持 哪个更好?比较了两者之后，我们可以说MySQL在改进自身以保持相关性方面做得很出色，但另一方面，对于PostgreSQL，不需要任何许可；它还提供表继承，规则系统，自定义数据类型和数据库事件。 因此可以说，PostgreSQL的边界肯定比MySQL高。 参考文档 本文译自 PostgreSQL vs MySQL: What’s the Difference?","link":"/2019/03/01/b7863297bbd9f2f618.html"},{"title":"MySQL集群实验","text":"2015年07月13日，官方发布了MySQL Cluster 7.4.7版本，趁着十一有空，根据官方文档的指引，基于VBox和Fedora Server，从0到1搭建MySQL Cluster集群的环境，记录搭建过程。 环境信息 服务器：Fedora Server 22-x86_64 Under VBox MySQL：MySQL Cluster(GPL) 7.4.7 linux glibc2.5 x86_64 具体版本：MySQL Cluster Community Server(GPL) 5.6.25-ndb-7.4.7-cluster-gpl 配置信息： Management ： 192.168.1.80 mgmt.dbsvr.com，用以管理集群 Storage Node： 192.168.1.81 ndb01.dbsvr.com，用作数据存储 Storage Node： 192.168.1.82 ndb02.dbsvr.com，用作数据存储 Storage Node： 192.168.1.83 ndb03.dbsvr.com，用作数据存储 SQL Node ： 192.168.1.84 sql01.dbsvr.com，用以执行SQL SQL Node ： 192.168.1.85 sql02.dbsvr.com，用以执行SQL Hosts配置： 各个MySQL Cluster节点的hosts(/etc/hosts)统一配置为： 192.168.1.80 mgmt.dbsvr.com # NDB Management Server192.168.1.81 ndb01.dbsvr.com # Data Node 01192.168.1.82 ndb02.dbsvr.com # Data Node 02192.168.1.83 ndb03.dbsvr.com # Data Node 03192.168.1.84 sql01.dbsvr.com # SQL Node 01192.168.1.85 sql02.dbsvr.com # SQL Node 02 安装过程由于实验环境在虚拟机中进行，虚拟机网络设置要使用桥接方式。可先安装好一个结点，然后再克隆虚拟机，修改Mac和IP地址，作为其他结点。如果全部在实体机上安装，可安装完成一台，然后其他机器再按照这台的安装经验来安装，避免出现其他问题。 安装好Fedora之后安装Vim、配置中科大镜像源，安装dnf取代yum，关闭防火墙，修改hosts(/etc/hosts)和hostname(/etc/hostname，修改后将在命令行中显示为[admin@hostname]$)。在/etc/sysconfig/network-scripts编辑ifcfg-enp0s3，将ip地址由dhcp动态分配改为static静态分配(注释部分为原内容，这里只列出修改和增加的部分内容)： 1234567891011[admin@mgmt]$ cd /etc/sysconfig/network-scripts[admin@mgmt]$ sudo vim ifcfg-enp0s3HWADDR=\"08:00:27:6E:39:CF\"#BOOTPROTO=\"dhcp\"BOOTPROTO=\"static\"#add by adminNM_CONTROLLED=\"yes\"IPADDR=\"192.168.1.80\"NETMASK=\"255.255.255.0\"GATEWAY=\"192.168.1.1\"DNS1=172.16.1.3 这里使用二进制安装方式，MySQL Cluster的安装与普通MySQL Server的安装并没有什么区别，区别只在于Cluster多了一些不一样的配置。 已事先将mysql-cluster-gpl-7.4.7-linux-glibc2.5-x86_64.tar.gz下载到~/目录下(使用虚拟机的情况下可以在服务内使用VSFTP搭建FTP服务器传文件)，安装过程参考解压后mysql-cluster-gpl-7.4.7-linux-glibc2.5-x86_64文件夹下的INSTALL-BINARY文件。这里是Fedora Server 22，为了避免后面的scripts/mysql_install_db --user=mysql执行出错，先安装perl： 1$ sudo dnf install -y perl-Module-Install.noarch 然后创建用户和组，解压，更改权限，开始安装MySQL Cluster(在一个具有Administrator权限的用户admin下)： 123456789[admin@mgmt]$ sudo groupadd mysql[admin@mgmt]$ sudo useradd -r -g mysql mysql[admin@mgmt]$ cd /usr/local[admin@mgmt]$ sudo tar xzvf ~/mysql-cluster-gpl-7.4.7-linux-glibc2.5-x86_64.tar.gz[admin@mgmt]$ sudo ln -s /usr/local mysql-cluster-gpl-7.4.7-linux-glibc2.5-x86_64 mysql[admin@mgmt]$ cd mysql[admin@mgmt]$ sudo chown -R mysql .[admin@mgmt]$ sudo chgrp -R mysql .[admin@mgmt]$ sudo scripts/mysql_install_db --user=mysql 如果mysql_install_db成功则会提示： 123New default config file was created as ./my.cnf andwill be used by default by the server when you start it.You may edit this file to change server settings 然后将当前目录(/usr/local/mysql)文件权限设置给root，但data即数据库文件目录设置给mysql： 123[admin@mgmt]$ chown -R root .[admin@mgmt]$ chown -R mysql data[admin@mgmt]$ sudo bin/mysqld_safe --user=mysql &amp; 检验是否安装成功，进入support-files，启动mysql server： 1234567891011121314151617181920212223242526[admin@mgmt]$ sudo ./mysql.server startStarting MySQL SUCCESS![admin@mgmt]$ cd ../bin[admin@mgmt]$ ./mysqlWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.6.25-ndb-7.4.7-cluster-gpl MySQL Cluster Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || test |+--------------------+2 rows in set (0.00 sec)mysql&gt; MySQL本地服务器安装成功，这就说明安装成功了，但是root账户还没设置密码，需要为root账户设置密码： 12345[admin@mgmt]$ pwd/usr/local/mysql/bin[admin@mgmt]$ ./mysqladmin -u root password \"123456\"Warning: Using a password on the command line interface can be insecure.[admin@mgmt]$ 以上是第一次设置root密码的时候，在命令行下设置，提示通过命令行接口设置密码不安全，不管它。另一个方法是进入mysql设置，这样会比较安全： 123456789101112131415161718[admin@mgmt]$ pwd/usr/local/mysql/bin[admin@mgmt]$ ./mysql -u rootWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 2Server version: 5.6.25-ndb-7.4.7-cluster-gpl MySQL Cluster Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; SET PASSWORD FOR 'root'@'localhost'=PASSWORD('123456');1 rows in set (0.00 sec)mysql&gt; 为了便于进入mysql，在用户配置文件中加入别名设置，这样就可以直接通过在任意地方键入mysql来进入mysql： 1234567[admin@mgmt]$ cd[admin@mgmt]$ ls -a. .bash_profile.. .bashrc.bash_history .mysql_history.bash_logout mysql-cluster-gpl-7.4.7-linux-glibc2.5-x86_64.tar.gz[admin@mgmt]$ vim ./.bashrc 然后在.bashrc中加入这一行alias mysql=/usr/local/bin/mysql，保存退出。 节点克隆由于各个MySQL Cluster结点是装在虚拟机中，所以直接使用VBox的虚拟机克隆操作就可以实现，克隆时要选择重新初始化网卡mac地址，以及选择Full Clone即完全克隆，这样会实际拷贝一份虚拟机。由于克隆后系统只是直接复制，所以需要根据VBox虚拟机网卡设置中新分配的MAC地址来修改克隆后的结点的MAC地址和IP地址，路径在： 123[admin@mgmt]$ sudo vim /etc/sysconfig/network-scripts/ifcfg-enp0s3HWADDR=\"08:00:27:2F:CF:AE\"IPADDR=\"192.168.1.81\" 通过如下命令来在工作站(我的是Mac，工作站的hosts也同一开始说的各个MySQL Cluster结点的hosts设置)中启动虚拟机结点(即MySQL集群服务器)，以及用ssh登陆(虚拟机网卡设置为桥接)： 1234Fury:~ Fury$ VBoxManage startvm mgmt.dbsvr.com -type vrdpWaiting for VM \"mgmt.dbsvr.com\" to power on...VM \"mgmt.dbsvr.com\" has been successfully started.Fury:~ Fury$ ssh -p 22 admin@mgmt.dbsvr.com 上面的startvm后的mgmt.dbsvr.com是虚拟机的名称，我这里恰好设置成了同结点的host domain，vrdp全称为Virtual box Remote Desktop Protocol。 结点配置在各个节点配置好之后首先要启动Management管理结点，然后启动Data Node数据节点和SQL Node查询结点(API结点)。 管理节点(MGM)配置管理节点即集群的中心，该节点需要记录其下有哪些数据节点和SQL节点，该节点的配置文件有两个，一个是my.cnf本身，一个是config.ini，其中config.ini需要包含入my.cnf当中。管理节点的my.cnf配置文件如下： 1234567891011# my.cnf# # example additions to my.cnf for MySQL Cluster (valid in MySQL 5.6)## # provide connection string for management server host (default port: 1186)[ndb_mgm]connect-string=mgmt.dbsvr.com## # provide location of cluster configuration file[ndb_mgmd]config-file=/etc/mysql/config.ini 在[ndb_mgm]部分通过connect-string指明管理节点的host，这里由于配置了hosts，所以直接写域名，也可以直接写ip地址，管理节点默认的链接端口为1186。在[ndb_mgmd]部分通过config-file来指明Cluster的全局配置文件，里面指明了该集群包括哪些数据节点和SQL节点。这里新建并放置config.ini文件于/etc/mysql/下，该全局配置文件的内容如下(需要自己创建/var/lib/mysql-cluster和/usr/local/mysql/mysql-cluster目录，关于NoOfReplicas请参考官方说明：NoOfReplicas)： 12345678910111213141516171819202122232425# file \"config.ini\" - 2 data nodes and 2 SQL nodes# This file is placed in the startup directory of ndb_mgmd (the# management server)# The first MySQL Server can be started from any host. The second# can be started only on the host mysqld_5.mysql.com[ndbd default]NoOfReplicas= 1DataDir=/usr/local/mysql/mysql-cluster[ndb_mgmd]Hostname= mgmt.dbsvr.comDataDir=/var/lib/mysql-cluster[ndbd]HostName=ndb01.dbsvr.com[ndbd]HostName=ndb02.dbsvr.com[ndbd]HostName=ndb03.dbsvr.com[mysqld]HostName=sql01.dbsvr.com[mysqld]HostName=sql02.dbsvr.com 由于mysql会寻找/etc/my.cnf来读取配置文件，所以可以通过如下命令来为my.cnf创建一个链接放到/etc/下： 123[admin@mgmt mysql]$ pwd/usr/local/mysql[admin@mgmt mysql]$ sudo ln -s /usr/local/mysql/my.cnf /etc/my.cnf 通过在/usr/local/mysql/bin/下使用ndb_mgmd(关于ndb_mgmd子程序的参数和作用参见命令$ ndb_mgmd --help)来启动管理节点： 123456[admin@mgmt mysql]$ pwd/usr/local/mysql[admin@mgmt mysql]$ sudo bin/ndb_mgmd[sudo] password for admin: MySQL Cluster Management Server mysql-5.6.25 ndb-7.4.7[admin@mgmt mysql]$ 通过如下命令来查看节点的连接情况(关于ndb_mgm子程序的参数和作用参见命令ndb_mgm --help)： 1234567891011121314151617[admin@mgmt mysql]$ bin/ndb_mgm -e showConnected to Management Server at: mgmt.dbsvr.com:1186Cluster Configuration---------------------[ndbd(NDB)] 3 node(s)id=2 (not connected, accepting connect from ndb01.dbsvr.com)id=3 (not connected, accepting connect from ndb02.dbsvr.com)id=4 (not connected, accepting connect from ndb03.dbsvr.com)[ndb_mgmd(MGM)] 1 node(s)id=1 @192.168.1.80 (mysql-5.6.25 ndb-7.4.7)[mysqld(API)] 2 node(s)id=5 (not connected, accepting connect from sql01.dbsvr.com)id=6 (not connected, accepting connect from sql02.dbsvr.com)[admin@mgmt mysql]$ 可以看到管理节点已经启动，地址为192.168.1.80，而三个数据节点和两个SQL节点均未连接过来。 数据节点(NDB)配置数据节点的配置相对管理节点要简单一些，它不需要config.ini，只需要在my.cnf中声明其本身为数据节点同时指定管理结点即可。同样地，为my.cnf创建一个链接放到/etc/下： 123[admin@mgmt mysql]$ pwd/usr/local/mysql[admin@mgmt mysql]$ sudo ln -s /usr/local/mysql/my.cnf /etc/my.cnf 在管理节点的config.ini中的[ndbd default]部分，我们通过DataDir来指定了数据节点的数据库文件存放位置为/usr/local/mysql/mysql-cluster，所以这里需要创建/usr/local/mysql/mysql-cluster目录。如下列出了my.cnf配置文件的内容： 123456789# For advice on how to change settings please see# http://dev.mysql.com/doc/refman/5.6/en/server-configuration-defaults.html[mysqld]ndbclusterndb-connectstring=mgmt.dbsvr.com[mysql_cluster]ndb-connectstring=mgmt.dbsvr.com 在[mysqld]部分使用ndbcluster来指明当前是数据节点，然后指明管理节点的地址，另外还需要[mysql_cluster]部分同时在该部分指明管理节点的地址。 使用ndbd子程序来启动数据节点，关于ndbd的参数请参考命令$ ndbd --help，使用如下命令来启动该数据节点： 123456[admin@ndb01 bin]$ pwd/usr/local/mysql/bin[admin@ndb01 bin]$ sudo ./ndbd --initial2015-10-03 11:15:52 [ndbd] INFO -- Angel connected to 'mgmt.dbsvr.com:1186'2015-10-03 11:15:52 [ndbd] INFO -- Angel allocated nodeid: 2[admin@ndb01 bin]$ 这说明该节点已经启动成功并且连接到了管理节点，它分配到的节点ID为2。回到管理节点进行查看： 1234567891011121314151617[admin@mgmt mysql]$ bin/ndb_mgm -e showConnected to Management Server at: mgmt.dbsvr.com:1186Cluster Configuration---------------------[ndbd(NDB)] 3 node(s)id=2 @192.168.1.81 (mysql-5.6.25 ndb-7.4.7, starting, Nodegroup: 0)id=3 (not connected, accepting connect from ndb02.dbsvr.com)id=4 (not connected, accepting connect from ndb03.dbsvr.com)[ndb_mgmd(MGM)] 1 node(s)id=1 @192.168.1.80 (mysql-5.6.25 ndb-7.4.7)[mysqld(API)] 2 node(s)id=5 (not connected, accepting connect from sql01.dbsvr.com)id=6 (not connected, accepting connect from sql02.dbsvr.com)[admin@mgmt mysql]$ 可以发现这个数据节点已经连接上了。使用同样方式配置和启动其他两个数据节点。 SQL节点(API)配置SQL节点的配置文件内容是同数据节点的是一样的，但是它由于不存储数据而只做查询，所以不需要创建/usr/local/mysql/mysql-cluster目录，只需要在安装好之后在my.cnf中同数据节点一样加入配置内容即可： 123456[mysqld]ndbclusterndb-connectstring=mgmt.dbsvr.com[mysql_cluster]ndb-connectstring=mgmt.dbsvr.com SQL节点的启动是启动MySQL Server，即： 12345[admin@sql01 support-files]$ pwd/usr/local/mysql/support-files[admin@sql01 support-files]$ sudo ./mysql.server startStarting MySQL.. SUCCESS! [admin@sql01 support-files]$ 以同样方式配置和启动另外一个SQL节点。 集群状态检查回到管理节点执行命令查看各个节点的连接情况： 1234567891011121314151617[admin@mgmt mysql]$ bin/ndb_mgm -e showConnected to Management Server at: mgmt.dbsvr.com:1186Cluster Configuration---------------------[ndbd(NDB)] 3 node(s)id=2 @192.168.1.81 (mysql-5.6.25 ndb-7.4.7, Nodegroup: 0, *)id=3 @192.168.1.82 (mysql-5.6.25 ndb-7.4.7, Nodegroup: 1)id=4 @192.168.1.83 (mysql-5.6.25 ndb-7.4.7, Nodegroup: 2)[ndb_mgmd(MGM)] 1 node(s)id=1 @192.168.1.80 (mysql-5.6.25 ndb-7.4.7)[mysqld(API)] 2 node(s)id=5 @192.168.1.84 (mysql-5.6.25 ndb-7.4.7)id=6 @192.168.1.85 (mysql-5.6.25 ndb-7.4.7)[admin@mgmt mysql]$ 可以发现所有的三个数据节点和两个SQL节点都已经连接上来了。接着就可以进行测试。 参考文档 Configuration of MySQL Cluster MySQL Cluster Configuration Files 集群测试使用集群需要指定引擎为NDBCLUSTER，这样才能使用集群功能，否则使用的只是节点的本地MySQL服务器。 执行测试进入sql01这个SQL节点，在test数据库中创建一个表并插入数据，然后到另外一个查询节点看看情况。分别在sql01和sql02中进入test数据库并查看表，发现为空： 12345678910111213141516171819202122232425262728[admin@sql01 ~]$ mysqlWelcome to the MySQL monitor. Commands end with ; or \\g.Your MySQL connection id is 3Server version: 5.6.25-ndb-7.4.7-cluster-gpl MySQL Cluster Community Server (GPL)Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.Oracle is a registered trademark of Oracle Corporation and/or itsaffiliates. Other names may be trademarks of their respectiveowners.Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || test |+--------------------+2 rows in set (0.00 sec)mysql&gt; use test;Database changedmysql&gt; show tables;Empty set (0.00 sec)mysql&gt; 在sql01结点中创建一个表并插入一条数据，注意一定要指定engine为ndbcluster： 123456789101112131415mysql&gt; create table cluster_test(id int) engine=ndbcluster;Query OK, 0 rows affected (0.23 sec)mysql&gt; insert into cluster_test(id) values(1010);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from cluster_test;+------+| id |+------+| 1010 |+------+1 row in set (0.00 sec)mysql&gt; 然后切换到sql02节点，重新show tables;看看情况： 1234567891011121314151617181920mysql&gt; show tables;Empty set (0.00 sec)mysql&gt; show tables;+----------------+| Tables_in_test |+----------------+| cluster_test |+----------------+1 row in set (0.00 sec)mysql&gt; select * from cluster_test;+------+| id |+------+| 1010 |+------+1 row in set (0.00 sec)mysql&gt; 发现在sql02节点当中已经可以看到在sql01当中创建的表了，查询也取出了数据。这说明该集群的基本配置是已经成功了。 参考文档 Quick Test Setup of MySQL Cluster","link":"/2015/10/03/63b2f7bb5a474de48.html"},{"title":"全民夺宝重构细节：幸运码的生成","text":"本文记录全民夺宝幸运码生成功能的重构优化。在全民夺宝项目中，每个商品上线会生成一期夺宝，一期夺宝完成之后生成新的一期供用户参与，系统需要自动为每一期生成幸运码。 原有情况原来的幸运码实现是大于10w元(豪车等)的手动随机生成并放置在文件中不存入MySQL(幸运码太多太大)，这类大额商品是手动发布期数的；小于10w元的自动生成并随机打乱，且不管最低参与价多少，都生成与商品售价相同数量的幸运码；生成的幸运码以JSON形式存储在商品期数表对应的一个字段中(MySQL)。 重构的目的是要实现： 不考虑商品价格区间全自动生成幸运码 根据最低参与价计算，减少幸运码数量 幸运码不再存储到MySQL，而是另外存储到MongoDB 幸运码的分配不再直接将号码与用户信息存储到MySQL表，而是以区间索引的方式记录到MongoDB 一个是改为自动化，一个是引入MongoDB存储幸运码，减少MySQL表数据量降低表读写压力。 需求梳理对于售价为P的商品，最小参与金额为U，其中：P为正整数(单位为元)，U为正整数且是P的因数(默认值为1)；即用户参与夺宝时，花费n*U的金额参与夺宝，可获得n个幸运码(重构后)。 幸运码定义：定为8位(千万)，取值范围为1000,000至9999,9999。那么，如何生成幸运码呢？ 问题分析商品某一期在上线时就需要确定售价P和最小参与金额为U，在这个基础上幸运码的数量N是确定的：N=P/U，一旦幸运码售完，就进入开奖环节。N个幸运码不能重复，且符合幸运码定义。有两种实现方式： 用户参与夺宝时再即时生成幸运码并分配 不容易保障幸运码唯一性 增加用户出价参与时的接口的耗时 上线新期数时提前生成，用户参与时进行分配 提前生成，随机打乱，用户参与时按顺序分配 很显然第一种并不可取，按第二种来实现：幸运码一旦生成就不再改变，为了在用户参与夺宝时提高幸运码的分配效率，幸运码串放置在一个数组中，按顺序分配给用户(只需要加锁控制并发分配即可)，这就要求在生成幸运码时产生的号码是随机的而不是连续的。这个问题最终简化为： 随机生成 N 个从 1 到 N且不重复的正整数 很显然一次遍历就可以生成1到N这N个不重复的正整数，而剩下的问题就是如何高效地随机打乱它：只需要再一次遍历，将当前位置的号码随机与其他号码中的某一个进行调换，就可以实现随机打乱它，而随机生成某个范围内的下标，有现成的库可以使用，这样我们就得到一个实现算法： 根据售价P和最小参与金额U计算幸运码的数量N 一次遍历按顺序生成1到N这N个正整数，不足8位时加上1000,000 一次遍历随机交换元素位置，以实现打乱生成的幸运码顺序 得到打乱顺序的幸运码，用户出价参与夺宝时按顺序分配幸运码即可 程序实现LuckyNumberBuilder.java 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566package any; // 避免暴露项目敏感细节，此处我替换掉import java.util.Random;public class LuckyNumberBuilder { public static final int LUCKY_NUMBER_MAX = 99999999; public static final int LUCKY_NUMBER_BASE = 10000000; // 1千万 private static void verifyPrice(int price, int unit) { if (unit &lt;= 0 || unit &gt; price || price &gt; LUCKY_NUMBER_MAX || price%unit != 0) { throw new IllegalArgumentException( \"售价(需为夺宝价的整数倍)或夺宝价无效，售价：\" + price + \"，夺宝价：\" + unit ); } } //该方法可以独立到公共包 public static void shuffle(int[] numbers) { if (numbers.length &lt;= 1) { return; } Random random = new Random(); //random.nextInt(10)%2 == 0 增强随机性 if (numbers.length == 2 &amp;&amp; random.nextInt(10)%2 == 0) { numbers[0] ^= numbers[1]; numbers[1] ^= numbers[0]; numbers[0] ^= numbers[1]; return; } for (int i = 0; i &lt; numbers.length; i++) { //随机选择一个 [0, N) 之间不为i的下标，注意溢出的情况 int si = random.nextInt(numbers.length); si = ((si == i) ? si + 1 : si) % numbers.length; numbers[i] ^= numbers[si]; numbers[si] ^= numbers[i]; numbers[i] ^= numbers[si]; } } public static int[] build(int price) { return build(price, 1); } public static int[] build(int price, int unit) { verifyPrice(price, unit); int luckyNumberCount = price / unit; int[] luckyNumbers = new int[luckyNumberCount]; for(int i=0; i&lt;luckyNumberCount; i++) { luckyNumbers[i] = i + 1; if (i+1 &lt; LUCKY_NUMBER_BASE) { luckyNumbers[i] += LUCKY_NUMBER_BASE; } } shuffle(luckyNumbers); return luckyNumbers; }} 效率分析两个for遍历N次，时间复杂度为O(2N)亦即O(n)级别，已经可以接受。 显而易见的是，价格越高，生成的幸运码数量就可能越多，幸运码数量和占用空间的大小是随着价格的升高而线性增加的，所以空间的复杂度也在O(n)级别，而随着N越大，占用空间就越多：一个幸运码为一个数字，占用4个字节，考虑一下几种情况下生成幸运码的内存消耗情况： 考虑一个商品价格千万或百万级别的情况，这种情况下商品可能是豪宅别墅、普通商品房、或者豪车，这类商品吸引力大，一般最低参与价格为1元，以让所有人有最大的意愿参与，那么幸运码将会有千万个，为最低参与价格为1元的商品生成幸运码所需要的内存为： 123(5000*10000 / 1) * 4 / 1000 / 1000 = 200 (MB) // 5kw商品，一般没有（树大招风）(1000*10000 / 1) * 4 / 1000 / 1000 = 40 (MB) // 1kw商品，一般也不上（树大招风）(500*10000 / 1) * 4 / 1000 / 1000 = 20 (MB) // 500w，超跑豪车，基本上是最大价格的商品 而考虑大多数常见的商品价格情况，基本处在万元级别以下： 123(10*10000 / 1) * 4 / 1000 = 400 (KB) // 10w，一般没有这个价格的商品(5*10000 / 1) * 4 / 1000 = 200 (KB) // 5w，高级相机、手表等，高配苹果产品等(10000 / 1) * 4 / 1000 = 40 (KB) // 1w，绝大多数商品处在这个价格级别,及低于 也就是说按照这个实现方案，大多数商品的换期生成抽奖号码所消耗的CPU和内存资源都会比较平稳，资源耗用量不高，而少数情况下，豪车等顶级商品（500w级别）换期时会有一个小波峰，但处在可接受范围。","link":"/2016/07/01/d6e189234c794124.html"},{"title":"使用Nginx/uWSGI部署Django项目","text":"在做一个OvpnSpider的iOS客户端程序，该程序依托于日本筑波大学的VPN中继服务项目提供的API。VPNGate是一个分布式的VPN服务器实验项目，它本身并不提供VPN服务器，而是由全球各地自愿者将自己的电脑自愿提供出来充当VPN服务器。我要做的OvpnSpider就是依托VPNGate提供的API获取一个连接这些分布式VPN服务器的OpenVPN配置文件列表，将这些列表经过测试筛选排序然后供OvpnSpider的iOS客户端使用。 为了快速开发，服务器端采用 Nginx + uWSGI + Python(Django)来搭建，这里记录下环境搭建的过程。开发环境：Mac Pro。 安装配置 Nginx下载官方 Nginx-1.9.5 版本。安装文档：Installation Doc 附加模块因为iOS现在全部要求与服务器端的链接采用https，所以在搭建Nginx的时候要开启对SSL的支持。根据上面安装文档的提示，下载Pcre-8.37、Zlib-1.2.8以及OpenSSL-1.0.2d。对于Pcre和Zlib采用三步曲安装即可，我采用默认安装路径(/usr/local)： 123$ ./configure$ make$ sudo make install 对于OpenSSL需要指定64位编译： 123$ ./config darwin64-x86_64-cc$ make$ sudo make install 然后就可以进行下一步，即编译安装Nginx。 编译安装Nginx将nginx-1.9.5.tar.gz、openssl-1.0.2d.tar.gz、pcre-8.37.tar.gz和zlib-1.2.8.tar.gz放入同一个目录下，然后全部在当前目录解压。进入nginx-1.9.5目录，使用命令来配置： 1$ ./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module --with-pcre=../pcre-8.37 --with-zlib=../zlib-1.2.8 在这里遇到不少问题，首先是开启SSL需要编译时指定--with-http_ssl_module，但是即便前面已经编译安装了OpenSSL，它也依旧提示找不到OpenSSL： 1234$ ./configure: error: SSL modules require the OpenSSL library.You can either do not enable the modules, or install the OpenSSL libraryinto the system, or build the OpenSSL library statically from the sourcewith nginx by using --with-openssl=&lt;path&gt; option. 没办法，只能像zlib一样指定OpenSSL，使用命令： 1$ ./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module --with-openssl=../openssl-1.0.2d --with-pcre=../pcre-8.37 --with-zlib=../zlib-1.2.8 好了，./configure没有出现警告，继续make，然后中途出现一个警告，是在configure OpenSSL的时候出现的： 1WARNING! If you wish to build 64-bit library, then you have to invoke './Configure darwin64-x86_64-cc' manually. You have about 5 seconds to press Ctrl-C to abort. 这是由于我的系统是64位的。如果忽略警告继续，最后将出现错误： 1234ld: symbol(s) not found for architecture x86_64clang: error: linker command failed with exit code 1 (use -v to see invocation)make[1]: * [objs/nginx] Error 1make: * [build] Error 2 没办法，只能想办法让它以darwin64-x86_64-cc替代darwin-i386-cc来编译OpenSSL，怎么办？到openssl-1.0.2d下修改config文件，打开该文件，通过查找发现上面的警告消息出现在557行： 123456789101112131415//这一行是554行ISA64=`(sysctl -n hw.optional.x86_64) 2&gt;/dev/null`if [ \"$ISA64\" = \"1\" -a -z \"$KERNEL_BITS\" ]; then echo \"WARNING! If you wish to build 64-bit library, then you have to\" echo \" invoke './Configure darwin64-x86_64-cc' *manually*.\" if [ \"$TEST\" = \"false\" -a -t 1 ]; then echo \" You have about 5 seconds to press Ctrl-C to abort.\" (trap \"stty `stty -g`\" 2 0; stty -icanon min 0 time 50; read waste) &lt;&amp;1 fifiif [ \"$ISA64\" = \"1\" -a \"$KERNEL_BITS\" = \"64\" ]; then OUT=\"darwin64-x86_64-cc\"else OUT=\"darwin-i386-cc\"fi ;; 继续往下10行就能发现我们可以将OUT=&quot;darwin-i386-cc&quot;换为OUT=&quot;darwin64-x86_64-cc&quot;，这样就可以忽略警告，又能默认以darwin64-x86_64-cc进行编译了。好，就这么干，改完之后再次运行： 1$ ./configure --sbin-path=/usr/local/nginx/nginx --conf-path=/usr/local/nginx/nginx.conf --pid-path=/usr/local/nginx/nginx.pid --with-http_ssl_module --with-openssl=../openssl-1.0.2d --with-pcre=../pcre-8.37 --with-zlib=../zlib-1.2.8 没问题，继续make，忽略警告让它继续，然后发现出现了错误： 12345678910$ cd ../openssl-1.0.2d \\$ &amp;&amp; if [ -f Makefile ]; then /Applications/Xcode.app/Contents/Developer/usr/bin/make clean; fi \\$ &amp;&amp; ./config --prefix=/Users/Fury/Downloads/nginx-1.9.5/../openssl-1.0.2d/.openssl no-shared \\$ &amp;&amp; /Applications/Xcode.app/Contents/Developer/usr/bin/make \\$ &amp;&amp; /Applications/Xcode.app/Contents/Developer/usr/bin/make install LIBDIR=libMakefile is older than Makefile.org, Configure or config.Reconfigure the source tree (via './config' or 'perl Configure'), please.make[2]: * [Makefile] Error 1make[1]: * [../openssl-1.0.2d/.openssl/include/openssl/ssl.h] Error 2make: *** [build] Error 2 很明显是因为我改动了config文件，导致Makefile与它原有的不一致了，怎么办呢？嗯，进入openssl-1.0.2d目录来手动执行一下./config来生成Makefile： 12$ cd ../openssl-1.0.2d$ ./config 好了，回到nginx-1.9.5目录继续执行make： 12$ cd ../nginx-1.9.5$ make 这次正确无误，执行install来安装即可： 1$ sudo make install 测试Nginx服务器启动Nginx，下面列出的是几个常用操作： 123$ sudo nginx # 启动$ sudo nginx -s reload # 重新加载配置文件# 关闭的话直接 ps aux|grep nginx 来 kill 掉进程即可 然后打开localhost能看到页面内容如下则说明安装成功： 12345Welcome to nginx!If you see this page, the nginx web server is successfully installed and working. Further configuration is required.For online documentation and support please refer to nginx.org.Commercial support is available at nginx.com.Thank you for using nginx. 配置Nginx此处和第6节已搬到文章基于openssl颁发自签名SSL证书中。 生成证书这一节参考了文档Nginx配置SSL证书部署HTTPS网站 安装uWSGI服务器uWSGI的官方文档地址在这里，在Nginx下配置uWSGI来部署Django的文档在这里，我没有从源码编译安装，而是使用pip安装的： 1$ sudo -H pip install uwsgi 在Django项目的根路径下新建一个web.ini配置文件，用它来支持uWSGI的启动： 12345678[uwsgi]chdir=/Users/Fury/sharework/MyGitRepos/AutoVPN/Server/fvpndmodule=fvpnd.wsgi:applicationmaster=Truevacuum=Truesocket=127.0.0.1:53020pidfile=/tmp/fvpnd-master.piddaemonize=/tmp/fvpnd-uwsgi.log 然后可以通过下面的命令来使用uWSGI运行这个Django项目： 1234$ pwd$ /Users/Fury/sharework/MyGitRepos/AutoVPN/Server/fvpnd$ uwsgi --ini web.ini[uWSGI] getting INI configuration from web.ini 使用Django创建项目Django是一个Python框架，简单强大易使用。可到 Django 官网下载安装稳定版。我这里使用了 pip 进行安装： 1$ sudo -H pip install django 使用 Django 创建项目： 1$ django-admin startproject fvpnd 使用 Django 创建APP： 1$ python manage.py startapp api 对 APP 进行配置修改 fvpnd/settings.py，加入新建的APP： 123456789101112131415161718192021INSTALLED_APPS = (# 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'api',)DATABASES = { 'default': { 'ENGINE': 'django.db.backends.mysql', 'NAME': 'fvpndb', #'USER': 'fvpnapp', #'PASSWORD': '123456', 'USER': 'fvpndba', 'PASSWORD': '123456', 'HOST': '127.0.0.1', 'PORT': '3306', }} 修改 fvpnd/urls.py：将原内容替换（删除了admin，加入APP） 123456from api import urls as api_urlsfrom django.conf.urls import include, urlurlpatterns = [ url(r'^api/', include(api_urls)),] 在 api/ 下新建urls.py： 123456from api import viewsfrom django.conf.urls import include, urlurlpatterns = [ url(r'^$', views.api),] 修改 api/views.py的内容为： 123456#coding:utf-8from django.shortcuts import renderfrom django.http import HttpResponse def api(request): return HttpResponse(u\"Just a test!\") 这样通过访问https://localhost/api/就可以定位到这个api函数，将其作为入口。 使用内置服务器测试Django 内置了一个轻量级的http服务器，通过命令来运行(migrate是与数据库相关的，第一次时运行即可)： 12$ python manage.py migrate$ python manage.py runserver 默认将启动在 http://127.0.0.1:8000 。由于修改了默认的数据库，使用了MySQL,可能会报错提示找不到 MySQLdb，需要先安装两个组件： 12$ brew install mysql-connector-c$ sudo -H pip install mysql-pyth Django会对数据库进行测试（通过创建表的方式），所以数据库用户需要有创建权限（后面再改回去就好）。 部署 Django 项目：关闭项目的settings.py下的debug选项，在项目目录下（与manage.py同一个目录，fvpnd/）创建web.ini配置文件（与上面uWSGI部分的是一样的），内容为： 12345678[uwsgi]chdir=/Users/Fury/sharework/MyGitRepos/AutoVPN/Server/fvpndmodule=fvpnd.wsgi:applicationmaster=Truevacuum=Truesocket=127.0.0.1:53020pidfile=/tmp/fvpnd-master.piddaemonize=/tmp/fvpnd-uwsgi.log 使用命令启动项目： 1$ uwsgi --ini web.ini 具体配置可有官方文档参考。","link":"/2016/01/12/7ee4c380575e4507a.html"},{"title":"Form提交事务的四种方式","text":"在做Oracle Form开发时有四种提交事务的方式，而不同的方式使用的环境及其实现的作用不尽相同，充分地了解清楚各种方式及其使用环境，对于编写出高质量的代码至关重要，这四种提交事务方式对比和列举说明如下。 四种提交方式commit_form;针对form上面的数据变动进行commit,对于代码中的类似update, insert语句也进行提交；如果form上面的数据变动和代码中的数据变动有冲突，最后以界面上的为准。 do_key(‘commit_form’);会首先寻找form下的triggers中的KEY-COMMIT这个trigger，并执行KEY-COMMIT中所写的代码。如果没有KEY-COMMIT这个trigger，则会针对form和代码一起提交。如果form上面的数据变动和代码中的数据变动有冲突，最后以界面上的为准。 commit;对form和数据库进行提交；如果form上面的数据变动和代码中的数据变动有冲突，最后以界面上的为准。 forms_ddl(‘commit’);只针对代码中的update, insert, delete语句进行提交,form上面的数据变动不提交。","link":"/2015/12/08/d94f19437f0746e099a.html"},{"title":"基于UTL_TCP实现FTP传输文件","text":"项目上在开发完成报表之后需要将报表内容格式化一份成为CSV文件并传送到外围服务器，以进行与其他系统的连携，实现方式是在本地服务器上生成CSV文件，然后利用FTP传送至其他服务器。之前有个Java的并发程序来实现这个FTP上传的功能，但移植之后一直问题不断，从稳哥(藏章博客，文章)那里得知有个PL/SQL程序包封装了UTL_TCP来实现这个功能，使用之后发现非常方便快捷。程序包下载：CHX_FTP_UTL.pck 编译程序包第一步很简单，就是将这个程序包编译到你的环境里。 使用方法UTL_FTP程序包的使用很简单，下面列举了使用的方法顺序： 1234CHX_FTP_UTL.Login(); -- 方法：登陆到FTP服务器；CHX_FTP_UTL.List(); -- 方法：列出服务器上的指定目录下的内容；CHX_FTP_UTL.Get(); -- 方法：从FTP服务器下载文件到本地；CHX_FTP_UTL.Put(); -- 方法：上传文件到服务器的指定目录下； 测试代码下载：Tester.sql Oracle Directories在Oracle Erp中目录是一种类似环境变量的东西，一般用在UTL_XXX程序包中比较多，例如UTL_TCP、UTL_FILE等等。举个例子，在使用UTL_FILE.Fcopy()函数时，变量中的目录路径那就不能写Dir目录字符串，而是要使用Directories的方式。Directories的方式就是指新建一个目录（Directory），这可以在Dba_Directories视图当中查询得到，而新建和删除Directories则类似新建和删除同义词、序列等等，如： 123456789--//创建Directories FILE_PATHCreate or Replace Directory FILE_PATH AS '/oracle/home/imchaser/tester';--//查询系统已有的DirectoriesSelect * From Dba_Directories;--//删除指定的Directories FILE_PATH Drop Directory FILE_PATH; 其中删除操作需要system账户，apps账户默认是没有权限的。","link":"/2015/12/26/674d696d60c44ee7b.html"},{"title":"快速开发美观的HTML报表","text":"在项目实施的过程当中，功能顾问与客户沟通需求出好需求文档，技术顾问按需求文档开发报表。有一些非严格性要求的报表客户可能只是拿来看看，对对帐对对数据，这类报表不需要输出成为PDF格式，只需要输出到HTML或者Excel当中进行查看，但同时客户又提供了一个美观可展示的模板，功能顾问往往在Excel当中做好这个模板然后提供在需求开发文档中，例如下图： 输出HTML报表如何让它输出到Excel之后还保持原样的美观呢？我们可以通过输出成HTML报表，这样既可以在浏览器中查看，也可以在Excel中查看，关键是用Excel打开这个HTML之后还能保持像原模板一样的美观，而不是四四方方的行列Table结构。快速开发的方法自然不是一行一行编写HTML代码和CSS样式(不少同事这么干，同寝室的哥们儿就是这么干的，然后开发这样一个报表他需要三天，其中一天用来写这个模板)。下面讲述一种我在项目中总结的快速开发这类HTML报表的方法，能极大地提高开发效率，让我们将主要精力和时间集中在报表的取数逻辑SQL的编写上面。 第一步：制作Html模板依据功能顾问给出的需求文档样例，先制作一个Html模板。多数功能顾问都会将模板的源文件(Excel)给你，或者是直接给你，或者是加载到Word需求文档里面。比较坑爹的功能顾问会搞个截图，告诉你做成这个样子，那么你最好去跟他拿到这个他制作的Excel源文档，以便你能直接拷贝。 首先要处理Excel里面的模板格式，例如该合并单元格的合并单元格，该居中的居中，可以在结果中循环输出结果的则删除其他多余部分，只留下一个Minimum的样例即可，即让你的模板最后变成所见即所得，然后剔除多余部分。这些操作全部在Excel当中完成。之后选中你的模板部分内容，在Excel当中选择** 文件 / 另存为 **HTML文件，注意在出现的选项当中选择保存你选中的部分即可，不需要保存整个Sheet页面，然后得到一个初步的HTML文件。 由于是从Excel当中另存生成的HTML页面，所以会有很多冗余代码数据，需要将这部分不影响模板显示的数据删除。凡是不影响HTML模板显示样式的代码都进行删除，然后将HTML代码里面的所有英文单引号替换成英文双引号，并确保HTML头为（不然可能浏览器解析有问题）： 1&lt;!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\"&gt; &lt;html xmlns=\"http://www.w3.org/1999/xhtml\"&gt; 以上操作在文本编辑器(例如Sublime Text)当中完成。制作好的HTML模板如下： 第二步：生成输出HTML的代码Oracle Erp里面有两张表，All_Source和Dba_Source，这两个表就是存储了你编译到系统里面的源代码，于是就可以将HTML代码粘贴到一个程序包Package（Chx_Html_Buffer.pck）里面，然后将这个Package编译到系统里面，这样我们就能够根据程序包名来将HTML代码查询出来，然后LOOP一下就可以将他们输出出来了，这部分可以写成一个小程序来完成这个输出工作（Chx_Wdb_Utl.pck）。再然后便可以将输出的代码Copy到我们要开发的报表里面，OK，输出模板的代码你就不用愁了，接下来你就可以专注于写你的业务逻辑PLSQL，然后看看哪一个输出在哪一格显示，稍微修改点儿代码就OK了。 将HTML代码放入Package中进行编译： 编写并编译程序脚本，实现从Dba_Source中读取刚才编译进去的HTML代码，以便输出生成HTML模板的PL/SQL代码。通过右键选择Test运行该程序： 填写参数，第一个为存储HTML代码的程序包的名称，第二个为该程序包里面HTML代码在程序包主体中开始的行数，第三个为倒数结束的行数，最后一个为即将在报表程序中使用的输出文本的过程名（即用来封装Fnd_File.Put_Line()函数的存储过程，此处为Out_Msg）： 运行输出的代码如下，直接将这部分代码复制粘贴到报表程序当中，就实现了输出HTML脚本生成跟需求所要求一样的美观的模板了，而后只需要将注意力集中于业务逻辑的实现即可。最后在定义并发程序的时候将输出类型选择为Html： 步骤简单总结即写代码生成代码 从需求文档(Excel模板)整理好HTML模板 编写PL/SQL脚本将HTML模板代码生成输出 将输出的代码拷贝到报表程序当中进行调整","link":"/2015/12/21/254796bcaedf4dfc9.html"},{"title":"快速获取Form LOV的查询SQL","text":"在项目的平时开发中我们经常会需要引用系统已有的值列表(LOV)，然后去做我们的客户化的开发。如果是客户化的那么还可能通过打开Form直接查看的方式来获取LOV查询SQL，但如果是标准Form特别是类似采购订单界面、销售订单界面等复杂的Form就没那么简单了，下面的方法提供了一种快速获取Form Lov值列表SQL语句的方式，以销售订单事务处理类型设置界面为例，获取【完成集】LOV的查询SQL。 操作路径： 销售职责(OM) &gt; 设置 &gt; 事务处理类型 &gt; 定义 第一步获得当前界面的Session ID。可以通过个性化显示消息的方式来打印Session_ID。 操作： 对【事务处理类型】界面实行个性化。 操作路径： 帮助 &gt; 诊断 &gt; 自定义代码 &gt; 个性化 个性化选择的参数和设置如下图，其中显示Session ID的语句为： 1=(SELECT t.sid FROM v$mystat t where rownum = 1) 个性化设置好之后不必保存，直接点击验证即可看到Session ID（这里的ID是1259）： 第二步切换到原界面（这里是事务处理设置界面），然后点击那个LOV，之后不要有其他界面操作： 然后通过上面查询到的Session ID，结合下面的SQL脚本就可以得到该LOV的查询语句了： 12345678910111213141516171819DECLARE --&gt;&gt;查询指定会话下的前一个SQL操作的SQL地址 --&gt;&gt;依据该SQL地址取得SQL的查询脚本 CURSOR cur_resp IS SELECT swn.sql_text FROM v$sqltext_with_newlines swn ,v$session vs WHERE swn.address = vs.prev_sql_addr AND vs.sid = 1259 --session_Id ORDER BY swn.piece ASC; v_scripts CLOB; BEGIN --&gt;&gt;拼接查询脚本得到 LOV 的查询语句 FOR rec IN cur_resp LOOP v_scripts := v_scripts || rec.sql_text; END LOOP; dbms_output.put_line(v_scripts); END; 得到的LOV查询SQL为： 12345678910111213141516171819--&gt;&gt;更改环境查询语言 ALTER SESSION SET NLS_LANGUAGE = 'SIMPLIFIED CHINESE'; --&gt;&gt;以下是获取到的 LOV 查询SQL SELECT display_name ,description ,item_type ,NAME ,MAX(version) FROM wf_activities_vl WHERE TYPE = 'PROCESS' AND nvl(end_date, SYSDATE) &gt;= SYSDATE AND item_type = 'OEOH' AND runnable_flag = 'Y' AND NAME NOT LIKE 'UPG_%' GROUP BY display_name ,description ,NAME ,item_type 执行结果如图所示：","link":"/2015/12/16/deaf5c5b339b47e2.html"},{"title":"点击按钮打开指定的文件夹","text":"项目中要求在一个平台中只允许应用文件夹而不允许执行删除、新建等其他文件夹操作，这些操作在另外的地方进行定义，可通过在该平台中使用form功能参数的形式来为按钮关联指定的文件夹，然后分配给用户，每个用户通过点击按钮或者在form打开的时候就自动Load一个定义好的文件夹，这里提供点击按钮打开指定文件夹的测试代码，将这部分代码添加到button的when-button-pressed触发器当中即可。另外在禁用文件夹的时候还要在触发器代码的末尾以及使用到App_Folder.Event('XXX'); 代码的触发器后面添加禁用文件夹菜单和打开文件夹按钮的代码。 Just a sample code: 12345678910111213141516171819202122232425262728---- 点击按钮打开定义好的文件夹,在button的when-button-pressed中添加代码：--------------------------------------------------------------App_Folder_Frd_Stmt('Entering app_folder_load_fldr.');Copy(Fnd_Utilities.Current_Language, 'global.app_folder_lang_id');Copy(App_Folder.Curr_Object, 'global.app_folder_object_id');Copy(To_Char(App_Folder.Curr_User_Id), 'global.app_folder_user_id');SELECT T.Folder_Id INTO App_Folder.Curr_Folder_Id FROM Fnd_Folders t WHERE T.Object = 'ORDER_LINES'--Folder define code AND T.Name = '柜身_NEW'; --Folder Name IF NOT App_Folder_Get_Folder_Cont(App_Folder.Curr_Folder_Id) THEN App_Folder_Get_Onscrn_List;END IF;App_Folder_Paint_List;Copy('OPEN-FOLDER', 'global.folder_action');Copy(To_Char(App_Folder.Curr_Folder_Id), 'global.folder_id');Execute_Trigger('folder_return_action'); App_Folder_Autoquery(Name_In('system.cursor_block'));App_Folder.Pending_Where_Clause := NULL;--App_Folder_Move_Cursor('1');App_Folder_Set_Instance_Values; 结果如图所示：","link":"/2015/12/15/a13565fdf8b04869a.html"},{"title":"用App_Multi实现Form行选择","text":"Oracle提供了App_Multi程序包（详见AppCore.pll）来支持在多记录行的FORM当中选择一条或者多条记录。通过按住Ctrl键来实现跨行选择记录，通过点击起始行，按住Shift键来连续选择多行，通过编辑/全选和编辑/撤销全选来实现全选和撤销全选。使用该API在选中记录时记录会高亮，Ctrl和Shift的支持是通过在WHEN-MOUSE-CLICK当中调用代码App_Multi.Event(WHEN-MOUSE-CLICK);来实现的，这里有个不好的地方就是你放开键盘按键再单击的时候前面的选择就会释放，所以在客户化的开发当中一般不这么做，换一种简单的方法就是增加一个复选框列来实现。 这是实现结果效果图： 下面是实现步骤和注意事项，最后奉上该例子的代码。 实现要求开发一个Form来测试App_Multi API，将选择的数据保存到数据库的表当中。 实现步骤创建一个表CUX_TEST_TEMP，用来保存数据，代码如下： 1234567891011121314151617-- Create table create table CUX_TEST_TEMP ( ID NUMBER, RESULT VARCHAR2(240) ) tablespace APPS_TS_CUX_DATA pctfree 10 initrans 1 maxtrans 255 storage ( initial 64K next 1M minextents 1 maxextents unlimited ); 创建一个视图CUX_APP_MUTIL_TEST_V，代码如下： 1234567CREATE OR REPLACE VIEW CUX_APP_MUTIL_TEST_V AS SELECT t.organization_id ,t.inventory_item_id ,t.segment1 ,t.description FROM mtl_system_items_b t WHERE rownum &lt; 20; 创建一个程序包Cux_App_Mutil_Test_Pkg，用来处理保存按钮逻辑，实现将选择的数据保存下来，代码如下： 123456789101112131415161718192021222324252627282930313233CREATE OR REPLACE PACKAGE cux_app_mutil_test_pkg IS -- Author : ADMINISTRATOR -- Created : 2014/9/17 10:41:46 -- Purpose : -- Public type declarations TYPE rec_test IS RECORD( organization_id NUMBER ,segment1 VARCHAR2(240)); -- Public constant declarations TYPE tbl_test IS TABLE OF rec_test INDEX BY BINARY_INTEGER; -- Public function and procedure declarations PROCEDURE save_data(p_tbl IN tbl_test); END cux_app_mutil_test_pkg; / CREATE OR REPLACE PACKAGE BODY cux_app_mutil_test_pkg IS PROCEDURE save_data(p_tbl IN tbl_test) IS BEGIN DELETE FROM cux_test_temp; FOR i IN 1 .. p_tbl.count LOOP INSERT INTO cux_test_temp VALUES (p_tbl(i).organization_id ,p_tbl(i).segment1); END LOOP; COMMIT; END; END cux_app_mutil_test_pkg; / 编写Form，实现多选操作需要注意的是SELECT_ALL和DESELECT_ALL触发器放在BLOCK级别的话要设置执行层次为Before，即优先于FORM级别的同名触发器执行。 01、增加触发器： 02、在FORM级别的WHEN-NEW-FORM-INSTANCE中定义记录组： 123456789101112131415161718192021222324DECLARE x_rec_group_id recordgroup; x_col_id_holder groupcolumn; x_rec_group_name VARCHAR2(30); BEGIN x_rec_group_name := 'MUTILTST_MULTI'; x_rec_group_id := find_group(x_rec_group_name); IF id_null(x_rec_group_id) THEN x_rec_group_id := create_group(x_rec_group_name); x_col_id_holder := add_group_column(x_rec_group_id ,'REC_NUM' ,number_column); END IF; x_col_id_holder := add_group_column(x_rec_group_id ,'ORGANIZATION_ID' ,number_column); x_col_id_holder := add_group_column(x_rec_group_id ,'SEGMENT1' ,char_column ,30); END; 03、实现MULTI_RETURN_ACTION触发器代码： 12345678910111213141516171819202122232425262728293031DECLARE x_action VARCHAR2(64); x_blockname VARCHAR2(30); x_rec_count VARCHAR2(3); BEGIN x_blockname := name_in('GLOBAL.APPCORE_MULTI_BLOCK'); x_action := name_in('GLOBAL.APPCORE_MULTI_ACTION'); --Check for all the events before checking for --the label change event because that gets set --for many of the other events as well. IF (x_action = 'KEY-CLRRREC') THEN clear_record; IF NOT (form_success) THEN RAISE form_trigger_failure; END IF; ELSIF (x_action = 'SELECT_ALL') THEN app_multi.event('SELECT_ALL', x_blockname); ELSIF (x_action = 'DESELECT_ALL') THEN app_multi.event('DESELECT_ALL', x_blockname); ELSIF (x_action = 'LABEL_CHANGE') THEN --Number of selected has changed fill_up_lines_rec_grp('SINGLE_CHANGE'); END IF; END; 04、修改SELECT_ALL触发器代码（执行菜单编辑/全选时会触发）： 1234567891011121314151617181920212223242526DECLARE x_group_id recordgroup; x_num_records NUMBER; rec_num INTEGER; l_groupname VARCHAR2(30); l_block_name VARCHAR2(30); BEGIN app_multi.event('SELECT_ALL'); l_block_name := :system.current_block; l_groupname := l_block_name || '_MULTI'; x_group_id := find_group(l_groupname); x_num_records := get_group_row_count(x_group_id); FOR i IN 1 .. x_num_records LOOP copy('Y', l_block_name || '.SELECT_LINE'); rec_num := get_group_number_cell(l_groupname || '.REC_NUM', i); fill_up_lines_rec_grp('ALL'); go_record(rec_num); END LOOP; first_record; END; 05、修改DESELECT_ALL代码（执行菜单编辑/全选时会触发）： 12345678910111213141516171819202122DECLARE x_group_id recordgroup; x_num_records NUMBER; x_curr_rec_num NUMBER; l_groupname VARCHAR2(30); l_block_name VARCHAR2(30); BEGIN l_block_name := :system.current_block; l_groupname := l_block_name || '_MULTI'; x_group_id := find_group(l_groupname); x_num_records := get_group_row_count(x_group_id); FOR i IN 1 .. x_num_records LOOP x_curr_rec_num := get_group_number_cell(l_groupname || '.REC_NUM', i); go_record(x_curr_rec_num); copy('N', l_block_name || '.SELECT_LINE'); END LOOP; app_multi.event('DESELECT_ALL'); first_record; END; 06、实现数据块勾选框ITEM的WHEN-CHECKBOX-CHANGED代码： 12345678910111213DECLARE cur_record NUMBER; cur_block VARCHAR2(80); BEGIN cur_block := name_in('SYSTEM.CURSOR_BLOCK'); cur_record := name_in('SYSTEM.CURSOR_RECORD'); IF name_in('MUTILTST.SELECT_LINE') = 'Y' THEN app_multi.select_records(cur_block, cur_record); ELSE app_multi.deselect_record(cur_block, cur_record); END IF; END; 07、实现保存按钮代码(将FORM的RECORDGROUP值放入我们自定义的RECORD TABLE TYPE中传给包)： 12345678910111213141516171819202122232425PROCEDURE process IS x_num_records NUMBER; l_step VARCHAR2(30); x_rec_group VARCHAR2(30); l_rec_table cux_app_mutil_test_pkg.tbl_test; BEGIN l_step := '001'; x_num_records := get_group_row_count(find_group('MUTILTST_MULTI')); --app_multi.get_group_count(blockname=&gt;'MUTILTST'); l_step := '002'; x_rec_group := 'MUTILTST_MULTI'; l_step := '003'; FOR i IN 1 .. x_num_records LOOP l_rec_table(i).organization_id := get_group_number_cell('MUTILTST_MULTI.ORGANIZATION_ID' ,i); l_rec_table(i).segment1 := get_group_char_cell('MUTILTST_MULTI.SEGMENT1' ,i); END LOOP; l_step := '004'; cux_app_mutil_test_pkg.save_data(p_tbl =&gt; l_rec_table); EXCEPTION WHEN OTHERS THEN fnd_message.debug(l_step); END; 08、fill_up_lines_rec_grp存储过程代码： 1234567891011121314151617181920212223242526272829303132333435PROCEDURE fill_up_lines_rec_grp(event IN VARCHAR2) IS x_num_records NUMBER; x_curr_rec_num NUMBER; l_beg_index NUMBER; rec_num INTEGER; l_cur_block VARCHAR2(30); l_group_name VARCHAR2(30); BEGIN l_cur_block := name_in('SYSTEM.CURSOR_BLOCK'); l_group_name := 'MUTILTST_MULTI'; x_num_records := get_group_row_count(find_group(l_group_name)); --fnd_message.debug(x_num_records); IF x_num_records &gt; 0 THEN IF event = 'ALL' THEN l_beg_index := 1; ELSE l_beg_index := x_num_records; END IF; FOR i IN l_beg_index .. x_num_records LOOP x_curr_rec_num := get_group_number_cell('MUTILTST_MULTI.REC_NUM', i); go_record(x_curr_rec_num); set_group_number_cell(l_group_name || '.ORGANIZATION_ID' ,i ,name_in(l_cur_block || '.ORGANIZATION_ID')); set_group_char_cell(l_group_name || '.SEGMENT1' ,i ,name_in(l_cur_block || '.SEGMENT1')); END LOOP; END IF; END; 参考资料 AppCore.pll / App_Multi 例子代码下载：App_Multi.zip 采购超级用户 &gt; 采购订单 &gt; 自动创建FORM源码 How do I use the APP_MULTI package to support multi-selection in my Oracle Form?","link":"/2015/12/10/f9767d6f26c145b.html"},{"title":"Forall用法及其异常捕获","text":"Oracle从8i开始引入了两个新的数据操纵语言语句：BULK COLLECT和FORALL，前者提供对数据的高速检索，后者则简化了代码同时很大程度上改进了增删改操作的性能。在Oracle EBS开发中经常需要批处理大量记录数据，例如在接口导入当中就使用得很多。下面的代码是我在实际项目开发当中在做客户化订单导入（从A系统导入到EBS）时使用FORALL的部分代码（下面的代码不支持直接Copy Running），在代码层面上简要地展示了FORALL的用法及其异常捕获机制，仅作参考Sample，具体的用法参见末尾的官方文档链接。 工作机制 Just a Sample code: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263DECLARE bulk_error EXCEPTION; --&gt;&gt;声明 Forall 异常,代码为 -24381 PRAGMA EXCEPTION_INIT(bulk_error, -24381); TYPE tbl_exc_type IS TABLE OF PLS_INTEGER INDEX BY PLS_INTEGER; l_exc_tbl tbl_exc_type; l_error_msg VARCHAR2(2000); l_iface_tbl cux_om_ordimp_define_pub.tbl_pub_header_iface;BEGIN l_exc_tbl.delete; FORALL idx IN 1 .. l_iface_tbl.count SAVE EXCEPTIONS --&gt;&gt;保存异常 INSERT INTO cux_om_so_headers_int VALUES (l_iface_tbl(idx).group_id ,l_iface_tbl(idx).source_code ,l_iface_tbl(idx).ordered_date ,l_iface_tbl(idx).project_number ,l_iface_tbl(idx).order_type ,l_iface_tbl(idx).customer_number ,l_iface_tbl(idx).operation_code ,l_iface_tbl(idx).operation_date ,l_iface_tbl(idx).batch_id ,l_iface_tbl(idx).process_code ,l_iface_tbl(idx).process_date ,l_iface_tbl(idx).error_msg ,l_iface_tbl(idx).currency_code ,l_iface_tbl(idx).creation_date ,l_iface_tbl(idx).created_by ,l_iface_tbl(idx).last_updated_by ,l_iface_tbl(idx).last_update_date ,l_iface_tbl(idx).last_update_login ,l_iface_tbl(idx).attribute_category ,l_iface_tbl(idx).attribute1 ,l_iface_tbl(idx).attribute2 ,l_iface_tbl(idx).attribute3 ,l_iface_tbl(idx).attribute4 ,l_iface_tbl(idx).attribute5 ,l_iface_tbl(idx).attribute6 ,l_iface_tbl(idx).attribute7 ,l_iface_tbl(idx).attribute8 ,l_iface_tbl(idx).attribute9 ,l_iface_tbl(idx).attribute10);EXCEPTION WHEN bulk_error THEN --&gt;&gt; 捕获异常 FOR i IN 1 .. SQL%bulk_exceptions.count LOOP --&gt;&gt;记录了错误时执行的行 l_exc_tbl(i) := SQL%BULK_EXCEPTIONS(i).error_index; l_error_msg := SQLERRM(-sql%BULK_EXCEPTIONS(i).error_code); l_iface_tbl(l_exc_tbl(i)).err_msg := substrb(l_error_msg ,1 ,2000); END LOOP; --&gt;&gt; 回写异常行 FOR i IN 1 .. l_excp_tbl.count LOOP UPDATE cux_ebs_control cec -- ctrl_id is Unique Idx SET cec.process_code = 'ERROR' ,cec.process_date = SYSDATE ,cec.error_msg = l_iface_tbl(l_exc_tbl(i)).err_msg WHERE cec.ctrl_id = l_iface_tbl(l_exc_tbl(i)).ctrl_id; END LOOP;END; 参考资料 Oracle Search FORALL Statement Usage Implicit Cursor Attribute","link":"/2015/12/09/2817051252954da2.html"},{"title":"线程间同步 - 轮流循环","text":"写程序实现: 主线程循环5次接着子线程循环3次，接着再主线程循环5次子线程循环3次，如此反复3趟。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package any;/** * 编写程序实现: 主线程循环5次接着子线程循环3次,接着再主线程循环5次子线程循环3次, * 如此反复循环3趟。分析：(5+3)*3=24, 子线程在loop为24时主线程在loop为21时退出. */class Looper implements Runnable { private volatile int turnsCounter = 0; // 线程自己的连续循环次数 private volatile int loopsCounter = 0; // 总循环次数 // firstRunFlag, 以true代表第一个执行循环的线程, false代表第二个执行循环的线程 private volatile boolean frf; public Looper(boolean frf){ this.frf = frf; } @Override public void run() { try { loop(false); // 从子线程执行 } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"Sub thread finished.\"); } // 定义一个循环(里面的while) public void loop(boolean isMainThread) throws InterruptedException { synchronized (this) { while (true) { // 在一开始就检查是否轮到自己执行循环, 轮不到就等待 waitTurns(isMainThread); // Working in loop ... turnsCounter++; System.out.println( Thread.currentThread().getName() + \", loop turn: \" + (loopsCounter + 1) ); // 循环次数统计 loopsCounter++; // 检查是否轮换循环以及是否可以停止自己的循环 if ((isMainThread &amp;&amp; turnsCounter == 5) || (!isMainThread &amp;&amp; turnsCounter == 3) ){ turnsCounter = 0; // 注意takeTurns里面反转了frf, 所以没有抽到if之前 if ((frf &amp;&amp; loopsCounter==21)||(!frf &amp;&amp; loopsCounter == 24)){ takeTurns(); break; //跳出循环,退出临界区,再接下来打印后线程结束 } else { takeTurns(); } } } } } private void waitTurns(boolean t) throws InterruptedException { while(t != this.frf){ this.wait(); } } private void takeTurns() { this.frf = !this.frf; this.notify(); //通知完成后在下一轮循环的waitTurns阻塞自己然后释放内部锁 }}public class Main { public static void main(String[] args) throws InterruptedException { Looper looper = new Looper(true); new Thread(looper).start(); // 先启动子线程 looper.loop(true); // 主线程开始进入循环 System.out.println(\"Main thread finished.\"); }} 注：网上有些实现很复杂，写了上百行代码，而且是各自代码循环，并不是同一个Loop里面交替循环；另外没搞清楚子线程的概念，完全是两个并行线程，都是主线程的子线程，共三个线程，是不对的。 输出： /Library/Java/JavaVirtualMachines/jdk1.8.0_241.jdk/Contents/Home/bin/java ……main, loop turn: 1main, loop turn: 2main, loop turn: 3main, loop turn: 4main, loop turn: 5Thread-0, loop turn: 6Thread-0, loop turn: 7Thread-0, loop turn: 8main, loop turn: 9main, loop turn: 10main, loop turn: 11main, loop turn: 12main, loop turn: 13Thread-0, loop turn: 14Thread-0, loop turn: 15Thread-0, loop turn: 16main, loop turn: 17main, loop turn: 18main, loop turn: 19main, loop turn: 20main, loop turn: 21Main thread finished.Thread-0, loop turn: 22Thread-0, loop turn: 23Thread-0, loop turn: 24Sub thread finished. Process finished with exit code 0","link":"/2016/03/06/43c600407ad34f9a.html"}],"tags":[{"name":"Java","slug":"Java","link":"/tags/Java/"},{"name":"自动拆装箱","slug":"自动拆装箱","link":"/tags/自动拆装箱/"},{"name":"包装类","slug":"包装类","link":"/tags/包装类/"},{"name":"CS","slug":"CS","link":"/tags/CS/"},{"name":"位运算","slug":"位运算","link":"/tags/位运算/"},{"name":"计算机体系结构","slug":"计算机体系结构","link":"/tags/计算机体系结构/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"分布式","slug":"分布式","link":"/tags/分布式/"},{"name":"日志追踪","slug":"日志追踪","link":"/tags/日志追踪/"},{"name":"C","slug":"C","link":"/tags/C/"},{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"内存","slug":"内存","link":"/tags/内存/"},{"name":"内存对齐","slug":"内存对齐","link":"/tags/内存对齐/"},{"name":"ArrayList","slug":"ArrayList","link":"/tags/ArrayList/"},{"name":"数组","slug":"数组","link":"/tags/数组/"},{"name":"白云山","slug":"白云山","link":"/tags/白云山/"},{"name":"生活","slug":"生活","link":"/tags/生活/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Ubuntu","slug":"Ubuntu","link":"/tags/Ubuntu/"},{"name":"GNome","slug":"GNome","link":"/tags/GNome/"},{"name":"源","slug":"源","link":"/tags/源/"},{"name":"FTP","slug":"FTP","link":"/tags/FTP/"},{"name":"SSH","slug":"SSH","link":"/tags/SSH/"},{"name":"RSA","slug":"RSA","link":"/tags/RSA/"},{"name":"SSL","slug":"SSL","link":"/tags/SSL/"},{"name":"DNS","slug":"DNS","link":"/tags/DNS/"},{"name":"CentOS","slug":"CentOS","link":"/tags/CentOS/"},{"name":"Hexo","slug":"Hexo","link":"/tags/Hexo/"},{"name":"博客","slug":"博客","link":"/tags/博客/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/tags/Kubernetes/"},{"name":"k8s","slug":"k8s","link":"/tags/k8s/"},{"name":"数据库","slug":"数据库","link":"/tags/数据库/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Postgres","slug":"Postgres","link":"/tags/Postgres/"},{"name":"集群","slug":"集群","link":"/tags/集群/"},{"name":"全民夺宝","slug":"全民夺宝","link":"/tags/全民夺宝/"},{"name":"随机","slug":"随机","link":"/tags/随机/"},{"name":"Django","slug":"Django","link":"/tags/Django/"},{"name":"Oracle","slug":"Oracle","link":"/tags/Oracle/"},{"name":"EBS","slug":"EBS","link":"/tags/EBS/"},{"name":"PLSQL","slug":"PLSQL","link":"/tags/PLSQL/"},{"name":"BitSet","slug":"BitSet","link":"/tags/BitSet/"},{"name":"Bitmap","slug":"Bitmap","link":"/tags/Bitmap/"},{"name":"位图","slug":"位图","link":"/tags/位图/"},{"name":"数据结构","slug":"数据结构","link":"/tags/数据结构/"},{"name":"Tomcat","slug":"Tomcat","link":"/tags/Tomcat/"},{"name":"Servlet","slug":"Servlet","link":"/tags/Servlet/"},{"name":"SpringMVC","slug":"SpringMVC","link":"/tags/SpringMVC/"},{"name":"线程","slug":"线程","link":"/tags/线程/"},{"name":"线程同步","slug":"线程同步","link":"/tags/线程同步/"}],"categories":[{"name":"Java","slug":"Java","link":"/categories/Java/"},{"name":"CS","slug":"CS","link":"/categories/CS/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"C/C++","slug":"C-C","link":"/categories/C-C/"},{"name":"Life","slug":"Life","link":"/categories/Life/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Miscs","slug":"Miscs","link":"/categories/Miscs/"},{"name":"Kubernetes","slug":"Kubernetes","link":"/categories/Kubernetes/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"Oracle","slug":"Oracle","link":"/categories/Oracle/"},{"name":"Spring","slug":"Spring","link":"/categories/Spring/"}]}